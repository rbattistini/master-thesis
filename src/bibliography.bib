@inproceedings{SilvaSP09,
  author       = {Lavindra de Silva and
                  Sebastian Sardi{\~{n}}a and
                  Lin Padgham},
  editor       = {Carles Sierra and
                  Cristiano Castelfranchi and
                  Keith S. Decker and
                  Jaime Sim{\~{a}}o Sichman},
  title        = {First principles planning in {BDI} systems},
  booktitle    = {8th International Conference on Autonomous Agents and Multiagent Systems
                  {(AAMAS} 2009), Budapest, Hungary, May 10-15, 2009, Volume 2},
  pages        = {1105--1112},
  publisher    = {{IFAAMAS}},
  year         = {2009},
  url          = {https://dl.acm.org/citation.cfm?id=1558167},
  timestamp    = {Tue, 05 Nov 2024 14:15:47 +0100},
  biburl       = {https://dblp.org/rec/conf/atal/SilvaSP09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{bdilogic-jlc8,
	author = {Rao, Anand S. and Georgeff, Michael P.},
	date-added = {2025-05-02 17:24:56 +0200},
	date-modified = {2025-05-02 17:24:56 +0200},
	doi = {10.1093/logcom/8.3.293},
	issn = {0955-792X},
	journal = {Journal of Logic and Computation},
	keywords = {Rational agents,belief-desire-intention (BDI) model,branching time temporal logic,modal logic,multi-modal logic,tableaux methods,temporal logic,theorem proving},
	month = jun,
	number = 3,
	pages = {293--342},
	title = {Decision procedures for {BDI} logics},
	url = {http://logcom.oxfordjournals.org/cgi/content/long/8/3/293},
	volume = 8,
	year = 1998,
	abstract = {The study of computational agents capable of rational behaviour has received increasing attention in recent years. A number of theoretical formalizations for such multi-agent systems have been proposed. However, most of these formalizations do not have a strong semantic basis nor a sound and complete axiomatization. Hence, it has not been clear as to how these formalizations could assist in building agents in practice. This paper explores a particular type of multi-agent system, in which each agent is viewed as having the three mental attitudes of belief (B), desire (D), and intention (I). It provides a family of multi-modal branching-time BDI logics with a possible-worlds semantics, categorizes them, provides sound and complete axiomatizations, and gives constructive tableau-based decision procedures for testing the satisfiability and validity of formulas. The computational complexity of these decision procedures is no greater than the complexity of their underlying temporal logic component.},
	bdsk-url-1 = {http://logcom.oxfordjournals.org/cgi/content/long/8/3/293},
	bdsk-url-2 = {http://dx.doi.org/10.1093/logcom/8.3.293}}

@inproceedings{AregbedeAPLL24,
	author = {Aregbede, Victor and Abraham, Savitha Sam and Persson, Andreas and L{\"a}ngkvist, Martin and Loutfi, Amy},
	booktitle = {2024 IEEE International Conference on Development and Learning (ICDL)},
	date-added = {2025-05-02 16:56:57 +0200},
	date-modified = {2025-05-02 16:56:57 +0200},
	doi = {10.1109/ICDL61372.2024.10644764},
	keywords = {Image segmentation;Large language models;Affordances;Focusing;Diffusion models;Task analysis;Robots},
	pages = {1-6},
	title = {Affordance-Based Goal Imagination for Embodied AI Agents},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1109/ICDL61372.2024.10644764}}

@article{HuangLWCWL24,
	author = {Huang, Xu and Liu, Weiwen and Chen, Xiaolong and Wang, Xingmei and Wang, Hao and Lian, Defu and Wang, Yasheng and Tang, Ruiming and Chen, Enhong},
	cdate = {1704067200000},
	date-added = {2025-05-02 16:52:33 +0200},
	date-modified = {2025-05-02 16:52:45 +0200},
	journal = {CoRR},
	publtype = {informal},
	title = {Understanding the planning of {LLM} agents: A survey},
	url = {https://doi.org/10.48550/arXiv.2402.02716},
	volume = {abs/2402.02716},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2402.02716}}

@inproceedings{RaoG95,
	address = {San Francisco, CA, USA},
	apice = {BdiIcmas95},
	author = {Rao, Anand S. and Georgeff, Michael P.},
	booktitle = {1st International Conference on Multi Agent Systems (ICMAS 1995)},
	date-added = {2025-05-02 16:24:38 +0200},
	date-modified = {2025-05-02 16:24:55 +0200},
	editor = {Lesser, Victor R. and Gasser, Les},
	isbn = {0-262-62102-9},
	month = {12-14~} # jun,
	pages = {312--319},
	publisher = {The MIT Press},
	title = {{BDI} Agents: From Theory to Practice},
	url = {https://www.aaai.org/Papers/ICMAS/1995/ICMAS95-042.pdf},
	year = 1995,
	bdsk-url-1 = {https://www.aaai.org/Papers/ICMAS/1995/ICMAS95-042.pdf}}

@incollection{rao-agentspeak96,
	apice = {RaoAgentspeak96},
	author = {Rao, Anand S.},
	booktitle = {Agents Breaking Away},
	date-added = {2025-05-02 16:23:40 +0200},
	date-modified = {2025-05-02 16:47:54 +0200},
	doi = {10.1007/BFb0031845},
	editor = {Van de Velde, Walter and Perram, John W.},
	eissn = {1611-3349},
	isbn = {978-3-540-60852-3},
	isbn10 = {3-540-60852-4},
	issn = {0302-9743},
	month = {22--25~} # jan,
	note = {7th European Workshop on Modelling Autonomous Agents in a Multi-Agent World (MAAMAW'96), Eindhoven, The Netherlands, 22-25~} # jan # {~1996, Proceedings},
	pages = {42--55},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	title = {{AgentSpeak(L)}: {BDI} Agents Speak Out in a Logical Computable Language},
	url = {http://link.springer.com/10.1007/BFb0031845},
	volume = 1038,
	year = 1996,
	bdsk-url-1 = {http://link.springer.com/10.1007/BFb0031845},
	bdsk-url-2 = {https://doi.org/10.1007/BFb0031845}}

@article{JaktaSNCS2024,
	_url = {https://doi.org/10.1007/s42979-024-03244-y},
	author = {Baiardi, Martina and Burattini, Samuele and Ciatto, Giovanni and Pianini, Danilo},
	doi = {10.1007/s42979-024-03244-y},
	journal = {Springer Nature Computer Science (SNCS)},
	number = {1003},
	title = {Blending {BDI} agents with object-oriented and functional programming with {JaKtA}},
	volume = {5},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s42979-024-03244-y}}

@article{MeneguzziZML07,
	author = {Meneguzzi, Felipe Rech and Zorzo, Avelino Francisco and da Costa M{\'{o}}ra, Michael and Luck, Michael},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/scpe/MeneguzziZML07.bib},
	journal = {Scalable Comput. Pract. Exp.},
	number = {1},
	timestamp = {Wed, 17 Feb 2021 22:02:06 +0100},
	title = {Incorporating Planning into {BDI} Systems},
	url = {http://www.scpe.org/index.php/scpe/article/view/394},
	volume = {8},
	year = {2007},
	bdsk-url-1 = {http://www.scpe.org/index.php/scpe/article/view/394}}

@inproceedings{MeneguzziZM04,
	author = {Meneguzzi, Felipe Rech and Zorzo, Avelino F. and da Costa M{\'{o}}ra, Michael},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/sac/MeneguzziZM04.bib},
	booktitle = {Proceedings of the 2004 {ACM} Symposium on Applied Computing (SAC), Nicosia, Cyprus, March 14-17, 2004},
	doi = {10.1145/967900.967916},
	editor = {Haddad, Hisham and Omicini, Andrea and Wainwright, Roger L. and Liebrock, Lorie M.},
	pages = {58--63},
	publisher = {{ACM}},
	timestamp = {Tue, 06 Nov 2018 11:06:45 +0100},
	title = {Propositional planning in {BDI} agents},
	url = {https://doi.org/10.1145/967900.967916},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1145/967900.967916}}

@inproceedings{BoselloR19,
	author = {Bosello, Michael and Ricci, Alessandro},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/emas/BoselloR19.bib},
	booktitle = {Engineering Multi-Agent Systems - 7th International Workshop, {EMAS} 2019, Montreal, QC, Canada, May 13-14, 2019, Revised Selected Papers},
	doi = {10.1007/978-3-030-51417-4\_9},
	editor = {Dennis, Louise A. and Bordini, Rafael H. and Lesp{\'{e}}rance, Yves},
	pages = {175--194},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {Thu, 06 Aug 2020 21:49:23 +0200},
	title = {From Programming Agents to Educating Agents - {A} Jason-Based Framework for Integrating Learning in the Development of Cognitive Agents},
	url = {https://doi.org/10.1007/978-3-030-51417-4\_9},
	volume = {12058},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-51417-4%5C_9}}

@article{MeneguzziS15,
	author = {Meneguzzi, Felipe and de Silva, Lavindra},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/ker/MeneguzziS15.bib},
	doi = {10.1017/S0269888913000337},
	journal = {Knowl. Eng. Rev.},
	number = {1},
	pages = {1--44},
	timestamp = {Thu, 27 Aug 2020 13:19:54 +0200},
	title = {Planning in {BDI} agents: a survey of the integration of planning algorithms and agent reasoning},
	url = {https://doi.org/10.1017/S0269888913000337},
	volume = {30},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1017/S0269888913000337}}

@inproceedings{XuLPM24,
	author = {Xu, Mengwei and Lumley, Tom and Pereira, Ramon Fraga and Meneguzzi, Felipe},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/ecai/XuLPM24.bib},
	booktitle = {{ECAI} 2024 - 27th European Conference on Artificial Intelligence, 19-24 October 2024, Santiago de Compostela, Spain - Including 13th Conference on Prestigious Applications of Intelligent Systems {(PAIS} 2024)},
	doi = {10.3233/FAIA240636},
	editor = {Endriss, Ulle and Melo, Francisco S. and Bach, Kerstin and Diz, Alberto Jos{\'{e}} Bugar{\'{\i}}n and Alonso{-}Moral, Jose Maria and Barro, Sen{\'{e}}n and Heintz, Fredrik},
	pages = {1365--1372},
	publisher = {{IOS} Press},
	series = {Frontiers in Artificial Intelligence and Applications},
	timestamp = {Mon, 03 Mar 2025 21:02:32 +0100},
	title = {A Practical Operational Semantics for Classical Planning in {BDI} Agents},
	url = {https://doi.org/10.3233/FAIA240636},
	volume = {392},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.3233/FAIA240636}}

@inproceedings{PereiraM24,
	author = {Pereira, Ramon Fraga and Meneguzzi, Felipe},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/atal/PereiraM24.bib},
	booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems, {AAMAS} 2024, Auckland, New Zealand, May 6-10, 2024},
	doi = {10.5555/3635637.3663257},
	editor = {Dastani, Mehdi and Sichman, Jaime Sim{\~{a}}o and Alechina, Natasha and Dignum, Virginia},
	pages = {2679--2683},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems / {ACM}},
	timestamp = {Wed, 26 Jun 2024 14:06:50 +0200},
	title = {Empowering {BDI} Agents with Generalised Decision-Making},
	url = {https://dl.acm.org/doi/10.5555/3635637.3663257},
	year = {2024},
	bdsk-url-1 = {https://dl.acm.org/doi/10.5555/3635637.3663257},
	bdsk-url-2 = {https://doi.org/10.5555/3635637.3663257}}

@inproceedings{IchidaM23,
	author = {Ichida, Alexandre Yukio and Meneguzzi, Felipe},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/sac/IchidaM23.bib},
	booktitle = {Proceedings of the 38th {ACM/SIGAPP} Symposium on Applied Computing, {SAC} 2023, Tallinn, Estonia, March 27-31, 2023},
	doi = {10.1145/3555776.3577657},
	editor = {Hong, Jiman and Lanperne, Maart and Park, Juw Won and Cern{\'{y}}, Tom{\'{a}}s and Shahriar, Hossain},
	pages = {856--863},
	publisher = {{ACM}},
	timestamp = {Fri, 21 Jul 2023 22:25:37 +0200},
	title = {Modeling a Conversational Agent using {BDI} Framework},
	url = {https://doi.org/10.1145/3555776.3577657},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3555776.3577657}}

@inproceedings{IchidaM024,
	author = {Ichida, Alexandre Yukio and Meneguzzi, Felipe and Cardoso, Rafael C.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/atal/IchidaM024.bib},
	booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems, {AAMAS} 2024, Auckland, New Zealand, May 6-10, 2024},
	doi = {10.5555/3635637.3662942},
	editor = {Dastani, Mehdi and Sichman, Jaime Sim{\~{a}}o and Alechina, Natasha and Dignum, Virginia},
	pages = {880--888},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems / {ACM}},
	timestamp = {Wed, 26 Jun 2024 14:06:50 +0200},
	title = {{BDI} Agents in Natural Language Environments},
	url = {https://dl.acm.org/doi/10.5555/3635637.3662942},
	year = {2024},
	bdsk-url-1 = {https://dl.acm.org/doi/10.5555/3635637.3662942},
	bdsk-url-2 = {https://doi.org/10.5555/3635637.3662942}}

@inproceedings{JangYCO023,
	author = {Jang, Minsu and Yoon, Youngwoo and Choi, Jaewoo and Ong, Hyobin and Kim, Jaehong},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/hai/JangYCO023.bib},
	booktitle = {International Conference on Human-Agent Interaction, {HAI} 2023, Gothenburg, Sweden, December 4-7, 2023},
	doi = {10.1145/3623809.3623930},
	pages = {375--377},
	publisher = {{ACM}},
	timestamp = {Sun, 10 Dec 2023 17:00:06 +0100},
	title = {A Structured Prompting based on Belief-Desire-Intention Model for Proactive and Explainable Task Planning},
	url = {https://doi.org/10.1145/3623809.3623930},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3623809.3623930}}

@article{DInvernoLGKW04,
	_url = {https://doi.org/10.1023/B:AGNT.0000019688.11109.19},
	author = {d'Inverno, Mark and Luck, Michael and Georgeff, Michael P. and Kinny, David and Wooldridge, Michael J.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/aamas/DInvernoLGKW04.bib},
	doi = {10.1023/B:AGNT.0000019688.11109.19},
	journal = {Auton. Agents Multi Agent Syst.},
	number = {1-2},
	pages = {5--53},
	timestamp = {Fri, 13 Mar 2020 10:56:03 +0100},
	title = {The dMARS Architecture: {A} Specification of the Distributed Multi-Agent Reasoning System},
	volume = {9},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1023/B:AGNT.0000019688.11109.19}}

@article{IngrandGR1992,
	author = {Ingrand, F.F. and Georgeff, M.P. and Rao, A.S.},
	doi = {10.1109/64.180407},
	journal = {IEEE Expert},
	number = {6},
	pages = {34-44},
	title = {An architecture for real-time reasoning and system control},
	volume = {7},
	year = {1992},
	bdsk-url-1 = {https://doi.org/10.1109/64.180407}}

@article{AdamGaudou2016,
	author = {Adam, Carole and Gaudou, Benoit},
	journal = {The Knowledge Engineering Review},
	number = {3},
	pages = {207--238},
	publisher = {Cambridge University Press},
	title = {{BDI} agents in social simulations: a survey},
	volume = {31},
	year = {2016}}

@incollection{HubnerB09,
	_editor = {Adelinde M. Uhrmacher and Danny Weyns},
	_publisher = {{CRC} Press / Taylor {\&} Francis},
	_series = {Computational Analysis, Synthesis, and Design of Dynamic Systems},
	_url = {https://doi.org/10.1201/9781420070248.ch15},
	author = {H{\"{u}}bner, Jomi Fred and Bordini, Rafael H.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/books/tf/09/HubnerB09.bib},
	booktitle = {Multi-Agent Systems - Simulation and Applications},
	doi = {10.1201/9781420070248.CH15},
	pages = {451--476},
	timestamp = {Mon, 15 Jul 2019 08:16:18 +0200},
	title = {Agent-Based Simulation Using {BDI} Programming in Jason},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1201/9781420070248.CH15}}

@book{BordiniHW2007,
	address = {Oxford, England},
	author = {Bordini, Rafael H and H{\"u}bner, Jomi Fred and Wooldridge, Michael},
	publisher = {John Wiley \& Sons},
	title = {Programming multi-agent systems in AgentSpeak using Jason},
	year = {2007}}

@book{BratmanEtAl1987,
	address = {Cambridge, MA},
	author = {Bratman, Michael and others},
	publisher = {Harvard University Press},
	title = {Intention, plans, and practical reason},
	volume = {10},
	year = {1987}}

@article{llmoracles-kbs310,
	apice = {LlmoraclesKbs310},
	articleno = 112940,
	arxiv = {2404.04108},
	author = {Ciatto, Giovanni and Agiollo, Andrea and Magnini, Matteo and Omicini, Andrea},
	dblp = {journals/kbs/CiattoAMO25},
	doi = {10.1016/j.knosys.2024.112940},
	iris = {11585/1001205},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Ontology population; Large language models; Nutrition; Automation; Domain-specific knowledge},
	month = {15~} # feb,
	numpages = 22,
	pages = {112940:1--22},
	publisher = {Elsevier B.V.},
	scholar = {12608317221311042342},
	scopus = {2-s2.0-85214522484},
	title = {Large language models as oracles for instantiating ontologies with domain-specific knowledge},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705124015740},
	urlopenaccess = {https://www.sciencedirect.com/science/article/pii/S0950705124015740},
	volume = 310,
	year = 2025,
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705124015740},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2024.112940}}

@incollection{imagination-extraamas2021,
	address = {Basel, Switzerland},
	author = {Ciatto, Giovanni and Najjar, Amro and Calbimonte, Jean-Paul and Calvaresi, Davide},
	booktitle = {Explainable and Transparent AI and Multi-Agent Systems. Third International Workshop, EXTRAAMAS 2021, Virtual Event, May 3--7, 2021, Revised Selected Papers},
	doi = {10.1007/978-3-030-82017-6_9},
	editor = {Calvaresi, Davide and Najjar, Amro and Winikoff, Michael and Fr{\"a}mling, Kary},
	eisbn = {978-3-030-82017-6},
	iris = {11585/842452},
	isbn = {978-3-030-82016-9},
	issn = {0302-9743},
	keywords = {Multi-agent systems; Imagination; BDI; Cognitive agents; XAI},
	pages = {139--157},
	publisher = {Springer Nature},
	series = {Lecture Notes in Computer Science},
	subseries = {Lecture Notes in Artificial Intelligence},
	title = {Towards Explainable Visionary Agents: License to Dare and Imagine},
	url = {http://link.springer.com/10.1007/978-3-030-82017-6_9},
	volume = 12688,
	year = 2021,
	bdsk-url-1 = {http://link.springer.com/10.1007/978-3-030-82017-6_9},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-82017-6_9}}

@inproceedings{HaoTTNSZXH23,
	_url = {https://doi.org/10.18653/v1/2023.findings-acl.309},
	author = {Hao, Shibo and Tan, Bowen and Tang, Kaiwen and Ni, Bin and Shao, Xiyan and Zhang, Hengzhe and Xing, Eric P. and Hu, Zhiting},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/acl/HaoTTNSZXH23.bib},
	booktitle = {Findings of the Association for Computational Linguistics: {ACL} 2023, Toronto, Canada, July 9-14, 2023},
	doi = {10.18653/V1/2023.FINDINGS-ACL.309},
	editor = {Rogers, Anna and Boyd{-}Graber, Jordan L. and Okazaki, Naoaki},
	pages = {5000--5015},
	publisher = {Association for Computational Linguistics},
	timestamp = {Thu, 10 Aug 2023 12:35:49 +0200},
	title = {BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.18653/V1/2023.FINDINGS-ACL.309}}

@article{ZhuWCQOYDCZ24,
	_url = {https://doi.org/10.1007/s11280-024-01297-w},
	author = {Zhu, Yuqi and Wang, Xiaohan and Chen, Jing and Qiao, Shuofei and Ou, Yixin and Yao, Yunzhi and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/www/ZhuWCQOYDCZ24.bib},
	doi = {10.1007/S11280-024-01297-W},
	journal = {World Wide Web {(WWW)}},
	number = {5},
	pages = {58},
	timestamp = {Mon, 02 Sep 2024 09:38:28 +0200},
	title = {{LLMs} for knowledge graph construction and reasoning: recent capabilities and future opportunities},
	volume = {27},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/S11280-024-01297-W}}

@article{PanRKSCDJO0LBMB23,
	_url = {https://doi.org/10.4230/TGDK.1.1.2},
	author = {Pan, Jeff Z. and Razniewski, Simon and Kalo, Jan{-}Christoph and Singhania, Sneha and Chen, Jiaoyan and Dietze, Stefan and Jabeen, Hajira and Omeliyanenko, Janna and Zhang, Wen and Lissandrini, Matteo and Biswas, Russa and de Melo, Gerard and Bonifati, Angela and Vakaj, Edlira and Dragoni, Mauro and Graux, Damien},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/tgdk/PanRKSCDJO0LBMB23.bib},
	doi = {10.4230/TGDK.1.1.2},
	journal = {{TGDK}},
	number = {1},
	pages = {2:1--2:38},
	timestamp = {Sun, 04 Aug 2024 19:52:22 +0200},
	title = {Large Language Models and Knowledge Graphs: Opportunities and Challenges},
	volume = {1},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.4230/TGDK.1.1.2}}

@inproceedings{KojimaGRMI22,
	address = {Red Hook, NY, USA},
	articleno = {1613},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems (NeurIPS 2022)},
	date-modified = {2024-07-22 18:07:09 +0200},
	isbn = {9781713871088},
	location = {New Orleans, LA, USA},
	numpages = {15},
	publisher = {Curran Associates Inc.},
	title = {Large language models are zero-shot reasoners},
	url = {https://openreview.net/pdf?id=e2TBb5y0yFf},
	year = {2024},
	bdsk-url-1 = {https://openreview.net/pdf?id=e2TBb5y0yFf}}

@article{RahmanKP18,
	author = {Rahman, Shafin and Khan, Salman and Porikli, Fatih},
	doi = {10.1109/TIP.2018.2861573},
	journal = {IEEE Transactions on Image Processing},
	number = {11},
	pages = {5652-5667},
	title = {A Unified Approach for Conventional Zero-Shot, Generalized Zero-Shot, and Few-Shot Learning},
	volume = {27},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TIP.2018.2861573}}

@article{roadmap-kg-2024,
	_url = {https://doi.org/10.1109/TKDE.2024.3352100},
	author = {Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/tkde/PanLWCWW24.bib},
	doi = {10.1109/TKDE.2024.3352100},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	number = {7},
	pages = {3580--3599},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	title = {Unifying Large Language Models and Knowledge Graphs: {A} Roadmap},
	volume = {36},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1109/TKDE.2024.3352100}}

@inproceedings{llm-as-encoders-shen-2023,
	author = {Shen, Jianhao and Wang, Chenguang and Gong, Linyuan and Song, Dawn},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/coling/Shen0GS22.bib},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics, {COLING} 2022, Gyeongju, Republic of Korea, October 12-17, 2022},
	editor = {Calzolari, Nicoletta and Huang, Chu{-}Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key{-}Sun and Ryu, Pum{-}Mo and Chen, Hsin{-}Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung{-}Hoon},
	pages = {1965--1978},
	publisher = {International Committee on Computational Linguistics},
	timestamp = {Thu, 13 Oct 2022 17:29:38 +0200},
	title = {Joint Language Semantic and Structure Embedding for Knowledge Graph Completion},
	url = {https://aclanthology.org/2022.coling-1.171},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.171}}

@article{llm-as-encoders-choi-2021,
	_url = {https://doi.org/10.1109/ACCESS.2021.3113329},
	author = {Choi, Bonggeun and Jang, Daesik and Ko, Youngjoong},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/access/ChoiJK21.bib},
	doi = {10.1109/ACCESS.2021.3113329},
	journal = {{IEEE} Access},
	pages = {132025--132032},
	timestamp = {Wed, 03 Nov 2021 08:25:53 +0100},
	title = {{MEM-KGC:} Masked Entity Model for Knowledge Graph Completion With Pre-Trained Language Model},
	volume = {9},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/ACCESS.2021.3113329}}

@inproceedings{llm-as-encoders-wang-2021,
	_url = {https://doi.org/10.1145/3442381.3450043},
	author = {Wang, Bo and Shen, Tao and Long, Guodong and Zhou, Tianyi and Wang, Ying and Chang, Yi},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/www/WangSLZW021.bib},
	booktitle = {{WWW} '21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021},
	doi = {10.1145/3442381.3450043},
	editor = {Leskovec, Jure and Grobelnik, Marko and Najork, Marc and Tang, Jie and Zia, Leila},
	pages = {1737--1748},
	publisher = {{ACM} / {IW3C2}},
	timestamp = {Sat, 30 Sep 2023 09:59:33 +0200},
	title = {Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3442381.3450043}}

@inproceedings{comet-2019,
	_url = {https://doi.org/10.18653/v1/p19-1470},
	author = {Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/acl/BosselutRSMCC19.bib},
	booktitle = {Proceedings of the 57th Conference of the Association for Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers},
	doi = {10.18653/V1/P19-1470},
	editor = {Korhonen, Anna and Traum, David R. and M{\`{a}}rquez, Llu{\'{\i}}s},
	pages = {4762--4779},
	publisher = {Association for Computational Linguistics},
	timestamp = {Sat, 29 Apr 2023 10:09:26 +0200},
	title = {{COMET:} Commonsense Transformers for Automatic Knowledge Graph Construction},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.18653/V1/P19-1470}}

@inproceedings{entity-discovery-decao-2021,
	author = {Cao, Nicola De and Izacard, Gautier and Riedel, Sebastian and Petroni, Fabio},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/iclr/CaoI0P21.bib},
	booktitle = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
	publisher = {OpenReview.net},
	timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
	title = {Autoregressive Entity Retrieval},
	url = {https://openreview.net/forum?id=5k8F6UU39V},
	year = {2021},
	bdsk-url-1 = {https://openreview.net/forum?id=5k8F6UU39V}}

@inproceedings{PetroniRRLBWM19,
	_url = {https://doi.org/10.18653/v1/D19-1250},
	author = {Petroni, Fabio and Rockt{\"{a}}schel, Tim and Riedel, Sebastian and Lewis, Patrick S. H. and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/emnlp/PetroniRRLBWM19.bib},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China, November 3-7, 2019},
	doi = {10.18653/V1/D19-1250},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	pages = {2463--2473},
	publisher = {Association for Computational Linguistics},
	timestamp = {Thu, 07 Apr 2022 09:14:07 +0200},
	title = {Language Models as Knowledge Bases?},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.18653/V1/D19-1250}}

@inproceedings{end-to-end-kg-kumar-2020,
	author = {Kumar, Abhijeet and Pandey, Abhishek and Gadia, Rohit and Mishra, Mridul},
	booktitle = {2020 IEEE International Conference on Computing, Power and Communication Technologies (GUCON)},
	doi = {10.1109/GUCON48875.2020.9231227},
	keywords = {Databases;Computational modeling;Conferences;Bit error rate;Text categorization;Pipelines;Buildings;knowledge graph;entity relations;relationship extraction;deep learning;BERT;language models;graph database},
	pages = {310-315},
	title = {Building Knowledge Graph using Pre-trained Language Model for Learning Entity-aware Relationships},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/GUCON48875.2020.9231227}}

@inproceedings{end-tp-end-kg-melnyk-2021,
	author = {Melnyk, Igor and Dognin, Pierre and Das, Payel},
	booktitle = {NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications},
	title = {Grapher: Multi-stage knowledge graph construction using pretrained language models},
	year = {2021}}

@article{end-to-end-kg-han-2023,
	_doi = {10.48550/ARXIV.2305.12392},
	_url = {https://doi.org/10.48550/arXiv.2305.12392},
	author = {Han, Jiuzhou and Collier, Nigel and Buntine, Wray L. and Shareghi, Ehsan},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2305-12392.bib},
	eprint = {2305.12392},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Fri, 26 May 2023 11:29:33 +0200},
	title = {PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs},
	volume = {abs/2305.12392},
	year = {2023}}

@inproceedings{entity-discovery-ayoola-2022,
	_url = {https://doi.org/10.18653/v1/2022.naacl-industry.24},
	author = {Ayoola, Tom and Tyagi, Shubhi and Fisher, Joseph and Christodoulopoulos, Christos and Pierleoni, Andrea},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/naacl/AyoolaTF0P22.bib},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track, {NAACL} 2022, Hybrid: Seattle, Washington, {USA} + Online, July 10-15, 2022},
	doi = {10.18653/V1/2022.NAACL-INDUSTRY.24},
	editor = {Loukina, Anastassia and Gangadharaiah, Rashmi and Min, Bonan},
	pages = {209--220},
	publisher = {Association for Computational Linguistics},
	timestamp = {Wed, 26 Oct 2022 19:40:34 +0200},
	title = {{ReFinED}: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.18653/V1/2022.NAACL-INDUSTRY.24}}

@inproceedings{kg-complenion-xin-2022,
	_url = {https://doi.org/10.1145/3487553.3524238},
	author = {Xie, Xin and Zhang, Ningyu and Li, Zhoubo and Deng, Shumin and Chen, Hui and Xiong, Feiyu and Chen, Mosha and Chen, Huajun},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/www/XieZLDCXCC22.bib},
	booktitle = {Companion of The Web Conference 2022, Virtual Event / Lyon, France, April 25 - 29, 2022},
	doi = {10.1145/3487553.3524238},
	editor = {Laforest, Fr{\'{e}}d{\'{e}}rique and Troncy, Rapha{\"{e}}l and Simperl, Elena and Agarwal, Deepak and Gionis, Aristides and Herman, Ivan and M{\'{e}}dini, Lionel},
	pages = {162--165},
	publisher = {{ACM}},
	timestamp = {Thu, 16 Feb 2023 16:21:15 +0100},
	title = {From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3487553.3524238}}

@article{kg-complenion-zhu-2023,
	_doi = {10.48550/ARXIV.2305.13168},
	_url = {https://doi.org/10.48550/arXiv.2305.13168},
	author = {Zhu, Yuqi and Wang, Xiaohan and Chen, Jing and Qiao, Shuofei and Ou, Yixin and Yao, Yunzhi and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2305-13168.bib},
	eprint = {2305.13168},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Tue, 30 May 2023 17:04:46 +0200},
	title = {{LLMs} for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities},
	volume = {abs/2305.13168},
	year = {2023}}

@inproceedings{kg-complenion-saxena-2022,
	_url = {https://doi.org/10.18653/v1/2022.acl-long.201},
	author = {Saxena, Apoorv and Kochsiek, Adrian and Gemulla, Rainer},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/acl/SaxenaKG22.bib},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022},
	doi = {10.18653/V1/2022.ACL-LONG.201},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	pages = {2814--2828},
	publisher = {Association for Computational Linguistics},
	timestamp = {Tue, 07 May 2024 20:08:12 +0200},
	title = {Sequence-to-Sequence Knowledge Graph Completion and Question Answering},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.18653/V1/2022.ACL-LONG.201}}

@inproceedings{kg-complenion-chen-2022,
	author = {Chen, Chen and Wang, Yufei and Li, Bing and Lam, Kwok{-}Yan},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/coling/ChenWLL22.bib},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics, {COLING} 2022, Gyeongju, Republic of Korea, October 12-17, 2022},
	editor = {Calzolari, Nicoletta and Huang, Chu{-}Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key{-}Sun and Ryu, Pum{-}Mo and Chen, Hsin{-}Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung{-}Hoon},
	pages = {4005--4017},
	publisher = {International Committee on Computational Linguistics},
	timestamp = {Tue, 19 Mar 2024 07:40:59 +0100},
	title = {Knowledge Is Flat: {A} Seq2Seq Generative Framework for Various Knowledge Graph Completion},
	url = {https://aclanthology.org/2022.coling-1.352},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.352}}

@inproceedings{lomov2020training,
	author = {Lomov, Pavel and Malozemova, Marina and Shishaev, Maxim},
	booktitle = {Software Engineering Perspectives in Intelligent Systems: Proceedings of 4th Computational Methods in Systems and Software 2020, Vol. 2 4},
	organization = {Springer},
	pages = {919--926},
	title = {Training and application of neural-network language model for ontology population},
	year = {2020}}

@inproceedings{kg-vs-ontology-2016,
	author = {Ehrlinger, Lisa and W{\"{o}}{\ss}, Wolfram},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/i-semantics/EhrlingerW16.bib},
	booktitle = {Joint Proceedings of the Posters and Demos Track of the 12th International Conference on Semantic Systems - SEMANTiCS2016 and the 1st International Workshop on Semantic Change {\&} Evolving Semantics (SuCCESS'16) co-located with the 12th International Conference on Semantic Systems (SEMANTiCS 2016), Leipzig, Germany, September 12-15, 2016},
	editor = {Martin, Michael and Cuquet, Mart{\'{\i}} and Folmer, Erwin},
	publisher = {CEUR-WS.org},
	series = {{CEUR} Workshop Proceedings},
	timestamp = {Fri, 10 Mar 2023 16:22:16 +0100},
	title = {Towards a Definition of Knowledge Graphs},
	url = {https://ceur-ws.org/Vol-1695/paper4.pdf},
	volume = {1695},
	year = {2016},
	bdsk-url-1 = {https://ceur-ws.org/Vol-1695/paper4.pdf}}

@misc{funk2023ontology,
	archiveprefix = {arXiv},
	author = {Funk, Maurice and Hosemann, Simon and Jung, Jean Christoph and Lutz, Carsten},
	eprint = {2309.09898},
	primaryclass = {cs.AI},
	title = {Towards Ontology Construction with Language Models},
	year = {2023}}

@incollection{expectation-extraamas2021,
	_url = {http://link.springer.com/10.1007/978-3-030-82017-6_20},
	address = {Basel, Switzerland},
	author = {Calvaresi, Davide and Ciatto, Giovanni and Najjar, Amro and Aydo{\u g}an, Reyhan and Van der Torre, Leon and Omicini, Andrea and Schumacher, Michael I.},
	booktitle = {Explainable and Transparent AI and Multi-Agent Systems. Third International Workshop, EXTRAAMAS 2021, Virtual Event, May 3--7, 2021, Revised Selected Papers},
	doi = {10.1007/978-3-030-82017-6_20},
	editor = {Calvaresi, Davide and Najjar, Amro and Winikoff, Michael and Fr{\"a}mling, Kary},
	iris = {11585/838535},
	isbn = {978-3-030-82016-9},
	isbn-online = {978-3-030-82017-6},
	issn = {0302-9743},
	keywords = {Multi-agent systems; eXplainable AI; CHIST-ERA IV; Personalisation; Decentralisation; Expectation},
	pages = {331--343},
	publisher = {Springer Nature},
	scopus = {2-s2.0-85113351710},
	series = {Lecture Notes in Computer Science},
	subseries = {Lecture Notes in Artificial Intelligence},
	title = {{{\sc Expectation}}: Personalized Explainable Artificial Intelligence for Decentralized Agents with Heterogeneous Knowledge},
	volume = 12688,
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-82017-6_20}}

@inproceedings{training-data-attack-2021,
	_author = {Carlini, Nicholas and Tram{\`{e}}r, Florian and Wallace, Eric and Jagielski, Matthew and Herbert{-}Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom B. and Song, Dawn and Erlingsson, {\'{U}}lfar and Oprea, Alina and Raffel, Colin},
	author = {Carlini, Nicholas and others},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/uss/CarliniTWJHLRBS21.bib},
	booktitle = {30th {USENIX} Security Symposium, {USENIX} Security 2021, August 11-13, 2021},
	date-modified = {2024-03-18 19:27:46 +0100},
	editor = {Bailey, Michael and Greenstadt, Rachel},
	pages = {2633--2650},
	publisher = {{USENIX} Association},
	timestamp = {Sun, 02 Oct 2022 16:16:46 +0200},
	title = {Extracting Training Data from Large Language Models},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting},
	year = {2021},
	bdsk-url-1 = {https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting}}

@inproceedings{bert-2019,
	author = {Devlin, Jacob and Chang, Ming{-}Wei and Lee, Kenton and Toutanova, Kristina},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)},
	doi = {10.18653/v1/n19-1423},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	pages = {4171--4186},
	publisher = {Association for Computational Linguistics},
	timestamp = {Mon, 26 Sep 2022 12:21:55 +0200},
	title = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.18653/v1/n19-1423}}

@article{Lamy17,
	_url = {https://doi.org/10.1016/j.artmed.2017.07.002},
	author = {Lamy, Jean{-}Baptiste},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/artmed/Lamy17.bib},
	date-modified = {2024-07-22 18:02:17 +0200},
	doi = {10.1016/J.ARTMED.2017.07.002},
	journal = {Artificial Intelligence in Medicine},
	pages = {11--28},
	timestamp = {Thu, 20 Feb 2020 15:45:57 +0100},
	title = {Owlready: Ontology-oriented programming in {P}ython with automatic classification and high level constructs for biomedical ontologies},
	volume = {80},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1016/J.ARTMED.2017.07.002}}

@article{Musen15,
	_url = {https://doi.org/10.1145/2757001.2757003},
	author = {Musen, Mark A.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/aimatters/Musen15.bib},
	date-modified = {2024-03-18 19:23:09 +0100},
	doi = {10.1145/2757001.2757003},
	journal = {{AI} Matters},
	number = {4},
	pages = {4--12},
	timestamp = {Wed, 14 Nov 2018 10:27:16 +0100},
	title = {The {Prot{\'e}g{\'e}} project: a look back and a look forward},
	volume = {1},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1145/2757001.2757003}}

@article{bernerslee2001semantic,
	added-at = {2016-09-07T01:44:07.000+0200},
	author = {Berners-Lee, Tim and Hendler, James and Lassila, Ora},
	biburl = {https://www.bibsonomy.org/bibtex/2222934145a71a9d6cfbbb375d4d62c1d/nosebrain},
	date-modified = {2024-07-22 17:55:43 +0200},
	interhash = {e87f09446138a81e6478625da97885b6},
	intrahash = {222934145a71a9d6cfbbb375d4d62c1d},
	journal = {Scientific American},
	lastdatemodified = {2007-04-27},
	lastname = {Berners-Lee},
	month = may,
	number = 5,
	own = {notown},
	pages = {34-43},
	read = {notread},
	timestamp = {2016-09-07T01:44:07.000+0200},
	title = {The Semantic Web},
	url = {https://www.scientificamerican.com/article/the-semantic-web/},
	volume = 284,
	year = 2001,
	bdsk-url-1 = {http://www.sciam.com/article.cfm?articleID=00048144-10D2-1C70-84A9809EC588EF21}}

@incollection{BaaderHS08,
	_url = {https://doi.org/10.1016/S1574-6526(07)03003-9},
	author = {Baader, Franz and Horrocks, Ian and Sattler, Ulrike},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/reference/fai/BaaderHS08.bib},
	booktitle = {Handbook of Knowledge Representation},
	doi = {10.1016/S1574-6526(07)03003-9},
	editor = {van Harmelen, Frank and Lifschitz, Vladimir and Porter, Bruce W.},
	pages = {135--179},
	publisher = {Elsevier},
	series = {Foundations of Artificial Intelligence},
	timestamp = {Mon, 06 Nov 2023 17:08:49 +0100},
	title = {Description Logics},
	volume = {3},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1016/S1574-6526(07)03003-9}}

@incollection{Grimm10,
	_url = {https://doi.org/10.1007/978-3-642-02788-8_6},
	author = {Grimm, Stephan},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/books/daglib/p/Grimm10.bib},
	booktitle = {Scientific Data Mining and Knowledge Discovery - Principles and Foundations},
	doi = {10.1007/978-3-642-02788-8_6},
	editor = {Gaber, Mohamed Medhat},
	pages = {111--137},
	publisher = {Springer},
	timestamp = {Mon, 29 May 2017 13:41:04 +0200},
	title = {Knowledge Representation and Ontologies},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-02788-8_6}}

@article{skeislr-acmcs,
	_url = {https://dl.acm.org/doi/10.1145/3645103},
	acm = {3645103},
	articleno = 161,
	author = {Ciatto, Giovanni and Sabbatini, Federico and Agiollo, Andrea and Magnini, Matteo and Omicini, Andrea},
	doi = {10.1145/3645103},
	eissn = {1557-734},
	issn = {0360-0300},
	journal = {ACM Computing Surveys},
	keywords = {Logic; Machine learning theory; Hybrid symbolic-numeric methods; Knowledge representation and reasoning},
	month = jun,
	number = 6,
	numpages = 35,
	pages = {161:1--161:35},
	publisher = {ACM},
	scholar = {13701373869146776438},
	semanticscholar = {267611660},
	title = {Symbolic Knowledge Extraction and Injection with Sub-symbolic Predictors: A Systematic Literature Review},
	urlopenaccess = {https://dl.acm.org/doi/pdf/10.1145/3645103},
	urlpdf = {https://dl.acm.org/doi/pdf/10.1145/3645103},
	volume = 56,
	year = 2024,
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3645103},
	bdsk-url-2 = {https://doi.org/10.1145/3645103},
	bdsk-url-3 = {https://dl.acm.org/doi/pdf/10.1145/3645103}}

@article{data-hungry-2014,
	author = {van der Ploeg, Tjeerd and Austin, Peter C and Steyerberg, Ewout W},
	journal = {BMC medical research methodology},
	number = {1},
	pages = {1--13},
	publisher = {BioMed Central},
	title = {Modern modelling techniques are data hungry: a simulation study for predicting dichotomous endpoints},
	volume = {14},
	year = {2014}}

@article{skerecommender-cmbp235,
	articleno = 107536,
	author = {Magnini, Matteo and Ciatto, Giovanni and Cant{\"u}rk, Furkan and Aydo{\v g}an, Reyhan and Omicini, Andrea},
	dblp = {journals/cmpb/MagniniCCAO23},
	doi = {10.1016/j.cmpb.2023.107536},
	iris = {11585/923772},
	issn = {0169-2607},
	journal = {Computer Methods and Programs in Biomedicine},
	keywords = {explainable artificial intelligence, symbolic knowledge extraction, recommendation systems, nutrition, neural networks},
	month = jun,
	numpages = 32,
	pubmed = {37060685},
	scholar = {14455392383017605572},
	scopus = {2-s2.0-85152230884},
	title = {Symbolic Knowledge Extraction for Explainable Nutritional Recommenders},
	volume = 235,
	year = 2023,
	bdsk-url-1 = {https://doi.org/10.1016/j.cmpb.2023.107536}}

@article{xaisurvey-ia14,
	_url = {http://content.iospress.com/articles/intelligenza-artificiale/ia190036},
	author = {Calegari, Roberta and Ciatto, Giovanni and Omicini, Andrea},
	booktitle = {Special Issue for the Twentieth Edition of the Workshop `From Objects to Agents'},
	date-added = {2021-05-20 10:40:06 +0200},
	date-modified = {2021-05-20 10:40:06 +0200},
	doi = {10.3233/IA-190036},
	editor = {Baldoni, Matteo and Bergenti, Federico and Monica, Stefania and Vizzari, Giuseppe},
	irisid = {11585/772707},
	journal = {Intelligenza Artificiale},
	keywords = {XAI, symbolic and sub-symbolic AI, explainability, interpretability, trustable system},
	number = 1,
	pages = {7--32},
	publisher = {IOS Press},
	scopusid = {2-s2.0-85092388493},
	title = {On the integration of symbolic and sub-symbolic techniques for {XAI}: A survey},
	volume = 14,
	wosid = {000574865700002},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.3233/IA-190036}}

@inproceedings{sparqldl-owled07,
	author = {Sirin, Evren and Parsia, Bijan},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/owled/SirinP07.bib},
	booktitle = {Proceedings of the {OWLED} 2007 Workshop on {OWL:} Experiences and Directions, Innsbruck, Austria, June 6-7, 2007},
	editor = {Golbreich, Christine and Kalyanpur, Aditya and Parsia, Bijan},
	publisher = {CEUR-WS.org},
	series = {{CEUR} Workshop Proceedings},
	timestamp = {Fri, 10 Mar 2023 16:23:02 +0100},
	title = {{SPARQL-DL:} {SPARQL} Query for {OWL-DL}},
	url = {https://ceur-ws.org/Vol-258/paper14.pdf},
	volume = {258},
	year = {2007},
	bdsk-url-1 = {https://ceur-ws.org/Vol-258/paper14.pdf}}

@inproceedings{MemmertCB24,
	author = {Memmert, Lucas and Cvetkovic, Izabel and Bittner, Eva A. C.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/hicss/MemmertCB24.bib},
	booktitle = {57th Hawaii International Conference on System Sciences, {HICSS} 2024, Hilton Hawaiian Village Waikiki Beach Resort, Hawaii, USA, January 3-6, 2024},
	editor = {Bui, Tung X.},
	pages = {7520--7529},
	publisher = {ScholarSpace},
	timestamp = {Thu, 04 Jan 2024 17:07:12 +0100},
	title = {The More Is Not the Merrier: Effects of Prompt Engineering on the Quality of Ideas Generated By {GPT-3}},
	url = {https://hdl.handle.net/10125/107289},
	year = {2024},
	bdsk-url-1 = {https://hdl.handle.net/10125/107289}}

@article{YangWLLZC23,
	_eprint = {2309.03409},
	_eprinttype = {arXiv},
	_url = {https://doi.org/10.48550/arXiv.2309.03409},
	author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2309-03409.bib},
	doi = {10.48550/ARXIV.2309.03409},
	journal = {CoRR},
	timestamp = {Tue, 12 Sep 2023 12:50:52 +0200},
	title = {Large Language Models as Optimizers},
	volume = {abs/2309.03409},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/ARXIV.2309.03409}}

@inproceedings{gpt3-2020,
	_author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert{-}Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	author = {Brown, Tom B. and others},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
	booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual},
	date-modified = {2024-03-18 19:28:26 +0100},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria{-}Florina and Lin, Hsuan{-}Tien},
	timestamp = {Thu, 25 May 2023 10:38:31 +0200},
	title = {Language Models are Few-Shot Learners},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}}

@inproceedings{ZhuKZSUTF15,
	_url = {https://doi.org/10.1109/ICCV.2015.11},
	author = {Zhu, Yukun and Kiros, Ryan and Zemel, Richard S. and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/iccv/ZhuKZSUTF15.bib},
	booktitle = {2015 {IEEE} International Conference on Computer Vision, {ICCV} 2015, Santiago, Chile, December 7-13, 2015},
	doi = {10.1109/ICCV.2015.11},
	pages = {19--27},
	publisher = {{IEEE} Computer Society},
	timestamp = {Thu, 23 Mar 2023 23:57:41 +0100},
	title = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/ICCV.2015.11}}

@article{roberta-2019,
	_eprint = {1907.11692},
	_eprinttype = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
	journal = {CoRR},
	timestamp = {Thu, 14 Dec 2023 18:03:41 +0100},
	title = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
	url = {http://arxiv.org/abs/1907.11692},
	volume = {abs/1907.11692},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1907.11692}}

@misc{gpt2-2019,
	author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	howpublished = {\url{https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf}},
	title = {Language Models are Unsupervised Multitask Learners},
	year = {2019}}

@misc{Gokaslan2019OpenWeb,
	author = {Gokaslan, Aaron and Cohen, Vanya},
	howpublished = {\url{http://Skylion007.github.io/OpenWebTextCorpus}},
	title = {OpenWebText Corpus},
	year = {2019}}

@article{TrieuQuoc2018,
	_eprint = {1806.02847},
	_eprinttype = {arXiv},
	author = {Trinh, Trieu H. and Le, Quoc V.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1806-02847.bib},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:46:22 +0200},
	title = {A Simple Method for Commonsense Reasoning},
	url = {http://arxiv.org/abs/1806.02847},
	volume = {abs/1806.02847},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1806.02847}}

@article{SchaefferMK23,
	_eprint = {2304.15004},
	_eprinttype = {arXiv},
	_url = {https://doi.org/10.48550/arXiv.2304.15004},
	author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2304-15004.bib},
	date-modified = {2024-03-18 19:06:03 +0100},
	doi = {10.48550/ARXIV.2304.15004},
	journal = {CoRR},
	timestamp = {Thu, 04 May 2023 15:47:42 +0200},
	title = {Are Emergent Abilities of Large Language Models a Mirage?},
	volume = {abs/2304.15004},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/ARXIV.2304.15004}}

@article{WuQRAKKA23,
	_eprint = {2307.02477},
	_eprinttype = {arXiv},
	_url = {https://doi.org/10.48550/arXiv.2307.02477},
	author = {Wu, Zhaofeng and Qiu, Linlu and Ross, Alexis and Aky{\"{u}}rek, Ekin and Chen, Boyuan and Wang, Bailin and Kim, Najoung and Andreas, Jacob and Kim, Yoon},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2307-02477.bib},
	doi = {10.48550/ARXIV.2307.02477},
	journal = {CoRR},
	timestamp = {Mon, 10 Jul 2023 17:27:00 +0200},
	title = {Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks},
	volume = {abs/2307.02477},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/ARXIV.2307.02477}}

@article{WeiTBRZBYBZMCHVLDF22,
	_author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	author = {Wei, Jason and others},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/tmlr/WeiTBRZBYBZMCHVLDF22.bib},
	date-modified = {2024-03-18 19:31:42 +0100},
	journal = {Trans. Mach. Learn. Res.},
	timestamp = {Fri, 19 May 2023 11:20:41 +0200},
	title = {Emergent Abilities of Large Language Models},
	url = {https://openreview.net/forum?id=yzkSU5zdwD},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://openreview.net/forum?id=yzkSU5zdwD}}

@article{llama1-2023,
	_author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie{-}Anne and Lacroix, Timoth{\'{e}}e and Rozi{\`{e}}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aur{\'{e}}lien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	_eprint = {2302.13971},
	_eprinttype = {arXiv},
	_url = {https://doi.org/10.48550/arXiv.2302.13971},
	author = {Touvron, Hugo and others},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2302-13971.bib},
	date-modified = {2024-03-18 19:17:09 +0100},
	doi = {10.48550/ARXIV.2302.13971},
	journal = {CoRR},
	timestamp = {Mon, 28 Aug 2023 21:26:20 +0200},
	title = {LLaMA: Open and Efficient Foundation Language Models},
	volume = {abs/2302.13971},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/ARXIV.2302.13971}}

@article{llama2-2023,
	_author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Canton{-}Ferrer, Cristian and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie{-}Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aur{\'{e}}lien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	_eprint = {2307.09288},
	_eprinttype = {arXiv},
	_url = {https://doi.org/10.48550/arXiv.2307.09288},
	author = {Touvron, Hugo and others},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2307-09288.bib},
	date-modified = {2024-03-18 19:16:30 +0100},
	doi = {10.48550/ARXIV.2307.09288},
	journal = {CoRR},
	timestamp = {Mon, 28 Aug 2023 21:26:22 +0200},
	title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
	volume = {abs/2307.09288},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/ARXIV.2307.09288}}

@article{RaffelSRLNMZLL20,
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
	journal = {J. Mach. Learn. Res.},
	pages = {140:1--140:67},
	timestamp = {Fri, 05 Feb 2021 15:43:41 +0100},
	title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	url = {http://jmlr.org/papers/v21/20-074.html},
	volume = {21},
	year = {2020},
	bdsk-url-1 = {http://jmlr.org/papers/v21/20-074.html}}

@techreport{Eurich2014TheBL,
	author = {Eurich, Markus and Burtscher, Michael Josef},
	date-modified = {2024-07-22 18:05:29 +0200},
	institution = {Cambridge Service Alliance},
	title = {The Business-to-Consumer Lock-in Effect},
	type = {Working paper},
	url = {https://cambridgeservicealliance.eng.cam.ac.uk/system/files/documents/2014AugustPaperBusinesstoConsumerLockinEffect.pdf},
	year = {2014},
	bdsk-url-1 = {https://cambridgeservicealliance.eng.cam.ac.uk/system/files/documents/2014AugustPaperBusinesstoConsumerLockinEffect.pdf}}

@article{hallucination-2023,
	_author = {Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and Wang, Longyue and Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming},
	_doi = {10.48550/arXiv.2309.01219},
	author = {Zhang, Yue and others},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2309-01219.bib},
	date-modified = {2024-03-18 19:32:31 +0100},
	eprint = {2309.01219},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 11 Sep 2023 16:01:35 +0200},
	title = {Siren's Song in the {AI} Ocean: {A} Survey on Hallucination in Large Language Models},
	volume = {abs/2309.01219},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2309.01219}}

@article{MinRSSVNAHR23,
	_url = {https://doi.org/10.1145/3605943},
	address = {New York, NY, USA},
	articleno = {30},
	author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heintz, Ilana and Roth, Dan},
	doi = {10.1145/3605943},
	issn = {0360-0300},
	issue_date = {February 2024},
	journal = {ACM Computing Surveys},
	keywords = {Large language models, neural networks, generative AI, foundational models},
	month = {sep},
	number = {2},
	numpages = {40},
	publisher = {Association for Computing Machinery},
	title = {Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey},
	volume = {56},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3605943}}

@article{gomes-2014,
	author = {de Faria, Carla Gomes and Serra, Ivo and Girardi, Rosario},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/scp/FariaSG14.bib},
	doi = {10.1016/j.scico.2013.12.005},
	journal = {Sci. Comput. Program.},
	pages = {26--43},
	timestamp = {Wed, 17 Feb 2021 21:56:11 +0100},
	title = {A domain-independent process for automatic ontology population from text},
	volume = {95},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1016/j.scico.2013.12.005}}

@incollection{petasis-2011,
	author = {Petasis, Georgios and Karkaletsis, Vangelis and Paliouras, Georgios and Krithara, Anastasia and Zavitsanos, Elias},
	booktitle = {Knowledge-Driven Multimedia Information Extraction and Ontology Evolution - Bridging the Semantic Gap},
	doi = {10.1007/978-3-642-20795-2_6},
	editor = {Paliouras, Georgios and Spyropoulos, Constantine D. and Tsatsaronis, George},
	pages = {134--166},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	title = {Ontology Population and Enrichment: State of the Art},
	volume = {6050},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-20795-2_6}}

@article{jannach-2009,
	author = {Jannach, Dietmar and Shchekotykhin, Kostyantyn M. and Friedrich, Gerhard},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/ws/JannachSF09.bib},
	doi = {10.1016/j.websem.2009.04.002},
	journal = {J. Web Semant.},
	number = {3},
	pages = {136--153},
	timestamp = {Sun, 02 Oct 2022 15:52:59 +0200},
	title = {Automated ontology instantiation from tabular web sources - The AllRight system},
	volume = {7},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1016/j.websem.2009.04.002}}

@inproceedings{ayadi-2019,
	author = {Ayadi, Ali and Samet, Ahmed and de Bertrand de Beuvron, Fran{\c{c}}ois and Zanni{-}Merk, Cecilia},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/kes/AyadiSBZ19.bib},
	booktitle = {Knowledge-Based and Intelligent Information {\&} Engineering Systems: Proceedings of the 23rd International Conference KES-2019, Budapest, Hungary, 4-6 September 2019},
	doi = {10.1016/j.procs.2019.09.212},
	editor = {Rudas, Imre J. and Csirik, J{\'{a}}nos and Toro, Carlos and Botzheim, J{\'{a}}nos and Howlett, Robert J. and Jain, Lakhmi C.},
	pages = {572--581},
	publisher = {Elsevier},
	series = {Procedia Computer Science},
	timestamp = {Mon, 28 Aug 2023 21:17:16 +0200},
	title = {Ontology population with deep learning-based {NLP:} a case study on the Biomolecular Network Ontology},
	volume = {159},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1016/j.procs.2019.09.212}}

@inproceedings{witte-2010,
	author = {Witte, Ren{\'{e}} and Khamis, Ninus and Rilling, Juergen},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/lrec/WitteKR10.bib},
	booktitle = {Proceedings of the International Conference on Language Resources and Evaluation, {LREC} 2010, 17-23 May 2010, Valletta, Malta},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Piperidis, Stelios and Rosner, Mike and Tapias, Daniel},
	publisher = {European Language Resources Association},
	timestamp = {Mon, 19 Aug 2019 15:22:48 +0200},
	title = {Flexible Ontology Population from Text: The OwlExporter},
	url = {http://www.lrec-conf.org/proceedings/lrec2010/summaries/932.html},
	year = {2010},
	bdsk-url-1 = {http://www.lrec-conf.org/proceedings/lrec2010/summaries/932.html}}

@article{corcoglioniti-2016,
	author = {Corcoglioniti, Francesco and Rospocher, Marco and Aprosio, Alessio Palmero},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/tkde/CorcoglionitiRA16.bib},
	doi = {10.1109/TKDE.2016.2602206},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	number = {12},
	pages = {3261--3275},
	timestamp = {Sat, 19 Oct 2019 19:10:59 +0200},
	title = {Frame-Based Ontology Population with {PIKES}},
	volume = {28},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TKDE.2016.2602206}}

@article{lubani-2019,
	author = {Lubani, Mohamed and Noah, Shahrul Azman Mohd. and Mahmud, Rohana},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/jis/LubaniNM19.bib},
	doi = {10.1177/0165551518801819},
	journal = {J. Inf. Sci.},
	number = {4},
	timestamp = {Fri, 27 Mar 2020 08:37:39 +0100},
	title = {Ontology population: Approaches and design aspects},
	volume = {45},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1177/0165551518801819}}

@article{cherifa-2021,
	author = {Khadir, Ahlem Ch{\'{e}}rifa and Aliane, Hassina and Guessoum, Ahmed},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/csr/KhadirAG21.bib},
	doi = {10.1016/j.cosrev.2020.100339},
	journal = {Comput. Sci. Rev.},
	pages = {100339},
	timestamp = {Tue, 02 Mar 2021 11:25:32 +0100},
	title = {Ontology learning: Grand tour and challenges},
	volume = {39},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1016/j.cosrev.2020.100339}}

@article{Alaswadi-2020,
	author = {Al{-}Aswadi, Fatima N. and Yong, Chan Huah and Gan, Keng Hoon},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/air/Al-AswadiYG20.bib},
	date-modified = {2024-07-22 17:59:42 +0200},
	doi = {10.1007/s10462-019-09782-9},
	journal = {Artificial Intelligence Review},
	number = {6},
	pages = {3901--3928},
	timestamp = {Thu, 14 Oct 2021 09:12:13 +0200},
	title = {Automatic ontology construction from text: a review from shallow to deep learning trend},
	volume = {53},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s10462-019-09782-9}}

@inproceedings{tanev-2006,
	author = {Tanev, Hristo and Magnini, Bernardo},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/eacl/TanevM06.bib},
	booktitle = {{EACL} 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference, April 3-7, 2006, Trento, Italy},
	editor = {McCarthy, Diana and Wintner, Shuly},
	publisher = {The Association for Computer Linguistics},
	timestamp = {Fri, 06 Aug 2021 00:40:45 +0200},
	title = {Weakly Supervised Approaches for Ontology Population},
	url = {https://aclanthology.org/E06-1003/},
	year = {2006},
	bdsk-url-1 = {https://aclanthology.org/E06-1003/}}

@inproceedings{yoon-2007,
	author = {Yoon, Hee{-}Geun and Han, Yong{-}Jin and Park, Seong{-}Bae and Park, Se{-}Young},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/alpit/YoonHPP07.bib},
	booktitle = {Proceedings of The Sixth International Conference on Advanced Language Processing and Web Information Technology, {ALPIT} 2007, Luoyang, Henan, China, 22-24 August 2007},
	doi = {10.1109/ALPIT.2007.30},
	pages = {135--139},
	publisher = {{IEEE} Computer Society},
	timestamp = {Fri, 24 Mar 2023 00:03:19 +0100},
	title = {Ontology Population from Unstructured and Semi-structured Texts},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/ALPIT.2007.30}}

@incollection{maynard-2008,
	_url = {http://www.booksonline.iospress.nl/Content/View.aspx?piid=8221},
	author = {Maynard, Diana and Li, Yaoyong and Peters, Wim},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/series/faia/MaynardLP08.bib},
	booktitle = {Ontology Learning and Population: Bridging the Gap between Text and Knowledge},
	date-modified = {2024-03-18 19:25:16 +0100},
	editor = {Buitelaar, Paul and Cimiano, Philipp},
	pages = {107--127},
	publisher = {{IOS} Press},
	series = {Frontiers in Artificial Intelligence and Applications},
	timestamp = {Mon, 11 Jul 2016 11:06:49 +0200},
	title = {{NLP} Techniques for Term Extraction and Ontology Population},
	url = {https://ebooks.iospress.nl/volume/proof-technology-and-computation},
	volume = {167},
	year = {2008},
	bdsk-url-1 = {http://www.booksonline.iospress.nl/Content/View.aspx?piid=8221}}

@article{jiang-2011,
	_author = {Jiang, Min and Chen, Yukun and Liu, Mei and Rosenbloom, S. Trent and Mani, Subramani and Denny, Joshua C. and Xu, Hua},
	author = {Jiang, Min and others},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/jamia/JiangCLRMDX11.bib},
	date-modified = {2024-03-18 19:19:12 +0100},
	doi = {10.1136/amiajnl-2011-000163},
	journal = {J. Am. Medical Informatics Assoc.},
	number = {5},
	pages = {601--606},
	timestamp = {Thu, 03 Sep 2020 15:09:17 +0200},
	title = {A study of machine-learning-based approaches to extract clinical entities and their assertions from discharge summaries},
	volume = {18},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1136/amiajnl-2011-000163}}

@inproceedings{celjuska-2004,
	author = {Celjuska, David and Vargas-Vera, Maria},
	booktitle = {International Conference on Natural Language Processing (ICON)},
	title = {Ontosophie: A semi-automatic system for ontology population from text},
	volume = {60},
	year = {2004}}

@article{etzioni-2005,
	author = {Etzioni, Oren and Cafarella, Michael J. and Downey, Doug and Popescu, Ana{-}Maria and Shaked, Tal and Soderland, Stephen and Weld, Daniel S. and Yates, Alexander},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/ai/EtzioniCDPSSWY05.bib},
	date-modified = {2024-07-22 18:01:30 +0200},
	doi = {10.1016/j.artint.2005.03.001},
	journal = {Artificial Intelligence},
	number = {1},
	pages = {91--134},
	timestamp = {Wed, 14 Nov 2018 10:50:00 +0100},
	title = {Unsupervised named-entity extraction from the Web: An experimental study},
	volume = {165},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1016/j.artint.2005.03.001}}

@article{souili-2015,
	author = {Souili, Achille and Cavallucci, Denis and Rousselot, Fran{\c{c}}ois},
	journal = {Procedia engineering},
	pages = {635--643},
	publisher = {Elsevier},
	title = {Natural language processing (NLP)--a solution for knowledge extraction from patent unstructured data},
	volume = {131},
	year = {2015}}

@inproceedings{finkestein-1999,
	author = {Finkelstein-Landau, Michal and Morin, Emmanuel},
	booktitle = {International Workshop on Ontological Engineering on the Global Information Infrastructure},
	date-modified = {2024-07-22 17:59:03 +0200},
	pages = {71--80},
	title = {Extracting Semantic Relationships between Terms: Supervised {\it vs.} Unsupervised Methods},
	year = {1999}}

@inproceedings{morin-1999,
	author = {Morin, Emmanuel},
	booktitle = {Proceedings of the Fifth International Congress on Terminology and Knowledge Engineering (TKE'99)},
	date-modified = {2024-07-22 18:00:27 +0200},
	title = {Automatic acquisition of semantic relations between terms from technical corpora},
	year = {1999}}

@article{harith-2003,
	author = {Harith, Alani and Kim, Sanghee and Millard, David E and Weal, Mark J and Hall, Wendy and Lewis, Paul H and Shadbolt, Nigel R},
	journal = {IEEE Intelligent Systems},
	number = {1},
	pages = {14--21},
	title = {Automatic ontology-based knowledge extraction and tailored biography generation from the web},
	volume = {18},
	year = {2003}}

@inproceedings{zeng-2014,
	author = {Zeng, Daojian and Liu, Kang and Lai, Siwei and Zhou, Guangyou and Zhao, Jun},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/coling/ZengLLZZ14.bib},
	booktitle = {{COLING} 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, August 23-29, 2014, Dublin, Ireland},
	editor = {Hajic, Jan and Tsujii, Junichi},
	pages = {2335--2344},
	publisher = {{ACL}},
	timestamp = {Fri, 06 Aug 2021 00:39:50 +0200},
	title = {Relation Classification via Convolutional Deep Neural Network},
	url = {https://aclanthology.org/C14-1220/},
	year = {2014},
	bdsk-url-1 = {https://aclanthology.org/C14-1220/}}

@inproceedings{liu-2013,
	author = {Liu, Chunyang and Sun, Wenbo and Chao, Wenhan and Che, Wanxiang},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/adma/LiuSCC13.bib},
	booktitle = {Advanced Data Mining and Applications - 9th International Conference, {ADMA} 2013, Hangzhou, China, December 14-16, 2013, Proceedings, Part {II}},
	doi = {10.1007/978-3-642-53917-6_21},
	editor = {Motoda, Hiroshi and Wu, Zhaohui and Cao, Longbing and Za{\"{\i}}ane, Osmar R. and Yao, Min and Wang, Wei},
	pages = {231--242},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {Tue, 01 Feb 2022 13:00:44 +0100},
	title = {Convolution Neural Network for Relation Extraction},
	volume = {8347},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-53917-6_21}}

@article{wang2023openchat,
	author = {Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
	journal = {arXiv preprint arXiv:2309.11235},
	title = {OpenChat: Advancing Open-source Language Models with Mixed-Quality Data},
	year = {2023}}

@article{mistral,
	_author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de Las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'{e}}lio Renard and Lachaux, Marie{-}Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'{e}}e and Sayed, William El},
	_eprint = {2310.06825},
	_eprinttype = {arXiv},
	_url = {https://doi.org/10.48550/arXiv.2310.06825},
	author = {Jiang, Albert Q. and others},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2310-06825.bib},
	date-modified = {2024-03-18 19:18:52 +0100},
	doi = {10.48550/ARXIV.2310.06825},
	journal = {CoRR},
	timestamp = {Thu, 26 Oct 2023 16:46:26 +0200},
	title = {Mistral 7B},
	volume = {abs/2310.06825},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/ARXIV.2310.06825}}

@article{gemini,
	_author = {Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean{-}Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and Millican, Katie and Silver, David and Petrov, Slav and Johnson, Melvin and Antonoglou, Ioannis and Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and Pitler, Emily and Lillicrap, Timothy P. and Lazaridou, Angeliki and Firat, Orhan and Molloy, James and Isard, Michael and Barham, Paul Ronald and Hennigan, Tom and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer, Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Tucker, George and Piqueras, Enrique and Krikun, Maxim and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and Roelofs, Becca and White, Ana{\"{\i}}s and Andreassen, Anders and von Glehn, Tamara and Yagati, Lakshman and Kazemi, Mehran and Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and et al.},
	_eprint = {2312.11805},
	_eprinttype = {arXiv},
	_url = {https://doi.org/10.48550/arXiv.2312.11805},
	author = {Anil, Rohan and others},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2312-11805.bib},
	date-modified = {2024-03-18 19:18:02 +0100},
	doi = {10.48550/ARXIV.2312.11805},
	journal = {CoRR},
	timestamp = {Tue, 16 Jan 2024 11:57:42 +0100},
	title = {Gemini: {A} Family of Highly Capable Multimodal Models},
	volume = {abs/2312.11805},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/ARXIV.2312.11805}}

@article{mixtral,
	_author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de Las Casas, Diego and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, L{\'{e}}lio Renard and Saulnier, Lucile and Lachaux, Marie{-}Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Th{\'{e}}ophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'{e}}e and Sayed, William El},
	_eprint = {2401.04088},
	_eprinttype = {arXiv},
	_url = {https://doi.org/10.48550/arXiv.2401.04088},
	author = {Jiang, Albert Q. and others},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2401-04088.bib},
	date-modified = {2024-03-18 19:18:36 +0100},
	doi = {10.48550/ARXIV.2401.04088},
	journal = {CoRR},
	timestamp = {Wed, 24 Jan 2024 16:19:32 +0100},
	title = {Mixtral of Experts},
	volume = {abs/2401.04088},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.48550/ARXIV.2401.04088}}

@article{gpt4,
	_eprint = {2303.08774},
	_eprinttype = {arXiv},
	_url = {https://doi.org/10.48550/arXiv.2303.08774},
	author = {OpenAI},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2303-08774.bib},
	doi = {10.48550/ARXIV.2303.08774},
	journal = {CoRR},
	timestamp = {Mon, 28 Aug 2023 21:26:19 +0200},
	title = {{GPT-4} Technical Report},
	volume = {abs/2303.08774},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/ARXIV.2303.08774}}

@article{ChenGptAndTime,
	_eprint = {2307.09009},
	_eprinttype = {arXiv},
	_url = {https://doi.org/10.48550/arXiv.2307.09009},
	author = {Chen, Lingjiao and Zaharia, Matei and Zou, James},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2307-09009.bib},
	doi = {10.48550/ARXIV.2307.09009},
	journal = {CoRR},
	timestamp = {Mon, 05 Feb 2024 20:18:49 +0100},
	title = {How is ChatGPT's behavior changing over time?},
	volume = {abs/2307.09009},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/ARXIV.2307.09009}}

@article{ZhaoTkde2022,
	_url = {https://doi.org/10.1109/TKDE.2020.3018741},
	author = {Zhao, Xiang and Zeng, Weixin and Tang, Jiuyang and Wang, Wei and Suchanek, Fabian M.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoZTWS22.bib},
	doi = {10.1109/TKDE.2020.3018741},
	journal = {{IEEE} Transactions on Knowledge and Data Engineering},
	number = {6},
	pages = {2610--2625},
	timestamp = {Thu, 27 Jul 2023 08:18:12 +0200},
	title = {An Experimental Study of State-of-the-Art Entity Alignment Approaches},
	volume = {34},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TKDE.2020.3018741}}

@inproceedings{XuAcl2019,
	_url = {https://doi.org/10.18653/v1/p19-1304},
	author = {Xu, Kun and Wang, Liwei and Yu, Mo and Feng, Yansong and Song, Yan and Wang, Zhiguo and Yu, Dong},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/acl/XuWYFSWY19.bib},
	booktitle = {Proceedings of the 57th Conference of the Association for Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers},
	doi = {10.18653/V1/P19-1304},
	pages = {3156--3161},
	publisher = {Association for Computational Linguistics},
	timestamp = {Thu, 12 May 2022 09:46:31 +0200},
	title = {Cross-lingual Knowledge Graph Alignment via Graph Matching Neural Network},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.18653/V1/P19-1304}}

@inproceedings{ChenIjcai2018,
	_url = {https://doi.org/10.24963/ijcai.2018/556},
	author = {Chen, Muhao and Tian, Yingtao and Chang, Kai{-}Wei and Skiena, Steven and Zaniolo, Carlo},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/ijcai/ChenTCSZ18.bib},
	booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, {IJCAI} 2018, July 13-19, 2018, Stockholm, Sweden},
	doi = {10.24963/IJCAI.2018/556},
	pages = {3998--4004},
	publisher = {ijcai.org},
	timestamp = {Tue, 20 Aug 2019 16:19:08 +0200},
	title = {Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.24963/IJCAI.2018/556}}

@inproceedings{LuanEmnlp2018,
	_url = {https://doi.org/10.18653/v1/d18-1360},
	author = {Luan, Yi and He, Luheng and Ostendorf, Mari and Hajishirzi, Hannaneh},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/emnlp/LuanHOH18.bib},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018},
	doi = {10.18653/V1/D18-1360},
	pages = {3219--3232},
	publisher = {Association for Computational Linguistics},
	timestamp = {Thu, 25 Apr 2024 15:20:37 +0200},
	title = {Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.18653/V1/D18-1360}}

@inproceedings{LieEacl2024,
	author = {Li, Dawei and Tan, Zhen and Chen, Tianlong and Liu, Huan},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/eacl/LiTCL24.bib},
	booktitle = {Findings of the Association for Computational Linguistics: {EACL} 2024, St. Julian's, Malta, March 17-22, 2024},
	pages = {458--477},
	publisher = {Association for Computational Linguistics},
	timestamp = {Tue, 02 Apr 2024 16:32:10 +0200},
	title = {Contextualization Distillation from Large Language Model for Knowledge Graph Completion},
	url = {https://aclanthology.org/2024.findings-eacl.32},
	year = {2024},
	bdsk-url-1 = {https://aclanthology.org/2024.findings-eacl.32}}

@article{FanIpm2024,
	_url = {https://doi.org/10.1016/j.ipm.2024.103646},
	author = {Fan, Zhanling and Chen, Chongcheng},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/ipm/FanC24.bib},
	doi = {10.1016/J.IPM.2024.103646},
	journal = {Inf. Process. Manag.},
	number = {2},
	pages = {103646},
	timestamp = {Fri, 19 Jul 2024 23:16:30 +0200},
	title = {CuPe-KG: Cultural perspective-based knowledge graph construction of tourism resources via pretrained language models},
	volume = {61},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1016/J.IPM.2024.103646}}

@article{BubeckCEHLKLPZ23,
	author = {Bubeck, S{\'{e}}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott M. and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco T{\'{u}}lio and Zhang, Yi},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2303-12712.bib},
	doi = {10.48550/ARXIV.2303.12712},
	eprint = {2303.12712},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 28 Aug 2023 21:26:19 +0200},
	title = {Sparks of Artificial General Intelligence: Early experiments with {GPT-4}},
	url = {https://doi.org/10.48550/arXiv.2303.12712},
	volume = {abs/2303.12712},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2303.12712}}

@inproceedings{ParkOCMLB23,
	author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/uist/ParkOCMLB23.bib},
	booktitle = {Proceedings of the 36th Annual {ACM} Symposium on User Interface Software and Technology, {UIST} 2023, San Francisco, CA, USA, 29 October 2023- 1 November 2023},
	doi = {10.1145/3586183.3606763},
	editor = {Follmer, Sean and Han, Jeff and Steimle, J{\"{u}}rgen and Riche, Nathalie Henry},
	pages = {2:1--2:22},
	publisher = {{ACM}},
	timestamp = {Sun, 19 Jan 2025 13:25:37 +0100},
	title = {Generative Agents: Interactive Simulacra of Human Behavior},
	url = {https://doi.org/10.1145/3586183.3606763},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3586183.3606763}}

@article{SumersYN024,
	author = {Sumers, Theodore R. and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/tmlr/SumersYN024.bib},
	journal = {Trans. Mach. Learn. Res.},
	timestamp = {Thu, 08 Aug 2024 15:22:39 +0200},
	title = {Cognitive Architectures for Language Agents},
	url = {https://openreview.net/forum?id=1i6ZCvflQJ},
	volume = {2024},
	year = {2024},
	bdsk-url-1 = {https://openreview.net/forum?id=1i6ZCvflQJ}}

@article{WangMFZYZCTCLZWW24,
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Jirong},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/fcsc/WangMFZYZCTCLZWW24.bib},
	doi = {10.1007/S11704-024-40231-1},
	journal = {Frontiers Comput. Sci.},
	number = {6},
	pages = {186345},
	timestamp = {Mon, 24 Feb 2025 10:41:45 +0100},
	title = {A survey on large language model based autonomous agents},
	url = {https://doi.org/10.1007/s11704-024-40231-1},
	volume = {18},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s11704-024-40231-1}}

@article{YangYH23,
	author = {Yang, Hui and Yue, Sifu and He, Yunzhong},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2306-02224.bib},
	doi = {10.48550/ARXIV.2306.02224},
	eprint = {2306.02224},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 12 Jun 2023 16:25:59 +0200},
	title = {Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions},
	url = {https://doi.org/10.48550/arXiv.2306.02224},
	volume = {abs/2306.02224},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2306.02224}}

@article{WangX0MXZFA24,
	author = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/tmlr/WangX0MXZFA24.bib},
	journal = {Trans. Mach. Learn. Res.},
	timestamp = {Thu, 08 Aug 2024 15:22:39 +0200},
	title = {Voyager: An Open-Ended Embodied Agent with Large Language Models},
	url = {https://openreview.net/forum?id=ehfRiF0R3a},
	volume = {2024},
	year = {2024},
	bdsk-url-1 = {https://openreview.net/forum?id=ehfRiF0R3a}}

@inproceedings{HazraMR24,
	author = {Hazra, Rishi and Martires, Pedro Zuidberg Dos and Raedt, Luc De},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/aaai/HazraMR24.bib},
	booktitle = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence, {AAAI} 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, {IAAI} 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2014, February 20-27, 2024, Vancouver, Canada},
	doi = {10.1609/AAAI.V38I18.29991},
	editor = {Wooldridge, Michael J. and Dy, Jennifer G. and Natarajan, Sriraam},
	pages = {20123--20133},
	publisher = {{AAAI} Press},
	timestamp = {Tue, 02 Apr 2024 16:32:09 +0200},
	title = {SayCanPay: Heuristic Planning with Large Language Models Using Learnable Domain Knowledge},
	url = {https://doi.org/10.1609/aaai.v38i18.29991},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1609/aaai.v38i18.29991}}

@inproceedings{DriessXSLCIWTVY23,
	author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/icml/DriessXSLCIWTVY23.bib},
	booktitle = {International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	pages = {8469--8488},
	publisher = {{PMLR}},
	series = {Proceedings of Machine Learning Research},
	timestamp = {Tue, 12 Nov 2024 16:50:49 +0100},
	title = {PaLM-E: An Embodied Multimodal Language Model},
	url = {https://proceedings.mlr.press/v202/driess23a.html},
	volume = {202},
	year = {2023},
	bdsk-url-1 = {https://proceedings.mlr.press/v202/driess23a.html}}

@inproceedings{YaoZYDSN023,
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik R. and Cao, Yuan},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/iclr/YaoZYDSN023.bib},
	booktitle = {The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
	publisher = {OpenReview.net},
	timestamp = {Wed, 24 Jul 2024 16:50:33 +0200},
	title = {ReAct: Synergizing Reasoning and Acting in Language Models},
	url = {https://openreview.net/forum?id=WE\_vluYUL-X},
	year = {2023},
	bdsk-url-1 = {https://openreview.net/forum?id=WE%5C_vluYUL-X}}

@inproceedings{ShinnCGNY23,
	author = {Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/nips/ShinnCGNY23.bib},
	booktitle = {Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023},
	editor = {Oh, Alice and Naumann, Tristan and Globerson, Amir and Saenko, Kate and Hardt, Moritz and Levine, Sergey},
	timestamp = {Fri, 01 Mar 2024 16:26:19 +0100},
	title = {Reflexion: language agents with verbal reinforcement learning},
	url = {http://papers.nips.cc/paper\_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html},
	year = {2023},
	bdsk-url-1 = {http://papers.nips.cc/paper%5C_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html}}

@inproceedings{winikoff2002kr,
	author = {Winikoff, Michael and Padgham, Lin and Harland, James and Thangarajah, John},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/kr/WinikoffPHT02.bib},
	booktitle = {Proceedings of the Eights International Conference on Principles and Knowledge Representation and Reasoning (KR-02), Toulouse, France, April 22-25, 2002},
	editor = {Fensel, Dieter and Giunchiglia, Fausto and McGuinness, Deborah L. and Williams, Mary{-}Anne},
	pages = {470--481},
	publisher = {Morgan Kaufmann},
	timestamp = {Tue, 22 Jul 2003 15:58:10 +0200},
	title = {Declarative {\&} Procedural Goals in Intelligent Agent Systems},
	year = {2002}}

@inproceedings{hubner2006dalt,
	author = {H{\"{u}}bner, Jomi Fred and Bordini, Rafael H. and Wooldridge, Michael J.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/dalt/HubnerBW06.bib},
	booktitle = {Declarative Agent Languages and Technologies IV, 4th International Workshop, {DALT} 2006, Hakodate, Japan, May 8, 2006, Selected, Revised and Invited Papers},
	doi = {10.1007/11961536\_9},
	editor = {Baldoni, Matteo and Endriss, Ulle},
	pages = {123--140},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {Tue, 29 Dec 2020 18:32:12 +0100},
	title = {Programming Declarative Goals Using Plan Patterns},
	url = {https://doi.org/10.1007/11961536\_9},
	volume = {4327},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1007/11961536%5C_9}}

@inproceedings{HindriksBHM00,
	author = {Hindriks, Koen V. and de Boer, Frank S. and van der Hoek, Wiebe and Meyer, John{-}Jules Ch.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/atal/HindriksBHM00.bib},
	booktitle = {Intelligent Agents {VII.} Agent Theories Architectures and Languages, 7th International Workshop, {ATAL} 2000, Boston, MA, USA, July 7-9, 2000, Proceedings},
	doi = {10.1007/3-540-44631-1\_16},
	editor = {Castelfranchi, Cristiano and Lesp{\'{e}}rance, Yves},
	pages = {228--243},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {Sun, 06 Oct 2024 20:56:35 +0200},
	title = {Agent Programming with Declarative Goals},
	url = {https://doi.org/10.1007/3-540-44631-1\_16},
	volume = {1986},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1007/3-540-44631-1%5C_16}}

@article{fikes1971ai,
	author = {Fikes, Richard and Nilsson, Nils J.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/ai/FikesN71.bib},
	doi = {10.1016/0004-3702(71)90010-5},
	journal = {Artif. Intell.},
	number = {3/4},
	pages = {189--208},
	timestamp = {Wed, 14 Nov 2018 10:50:00 +0100},
	title = {{STRIPS:} {A} New Approach to the Application of Theorem Proving to Problem Solving},
	url = {https://doi.org/10.1016/0004-3702(71)90010-5},
	volume = {2},
	year = {1971},
	bdsk-url-1 = {https://doi.org/10.1016/0004-3702(71)90010-5}}

@article{georgievski2015ai,
	author = {Georgievski, Ilche and Aiello, Marco},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/ai/Georgievski015.bib},
	doi = {10.1016/J.ARTINT.2015.02.002},
	journal = {Artif. Intell.},
	pages = {124--156},
	timestamp = {Wed, 25 Sep 2019 18:00:46 +0200},
	title = {{HTN} planning: Overview, comparison, and beyond},
	url = {https://doi.org/10.1016/j.artint.2015.02.002},
	volume = {222},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1016/j.artint.2015.02.002}}

@article{Murugesan25a,
  author       = {San Murugesan},
  title        = {The Rise of Agentic {AI:} Implications, Concerns, and the Path Forward},
  journal      = {{IEEE} Intell. Syst.},
  volume       = {40},
  number       = {2},
  pages        = {8--14},
  year         = {2025},
  url          = {https://doi.org/10.1109/MIS.2025.3544940},
  doi          = {10.1109/MIS.2025.3544940},
  timestamp    = {Fri, 02 May 2025 19:40:21 +0200},
  biburl       = {https://dblp.org/rec/journals/expert/Murugesan25a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{TangZPZCG23,
  author       = {Xiangru Tang and
                  Yiming Zong and
                  Jason Phang and
                  Yilun Zhao and
                  Wangchunshu Zhou and
                  Arman Cohan and
                  Mark Gerstein},
  title        = {Struc-Bench: Are Large Language Models Really Good at Generating Complex
                  Structured Data?},
  journal      = {CoRR},
  volume       = {abs/2309.08963},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.08963},
  doi          = {10.48550/ARXIV.2309.08963},
  eprinttype    = {arXiv},
  eprint       = {2309.08963},
  timestamp    = {Sun, 06 Oct 2024 21:24:07 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-08963.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{KongZCLQSSZ23,
  author       = {Aobo Kong and
                  Shiwan Zhao and
                  Hao Chen and
                  Qicheng Li and
                  Yong Qin and
                  Ruiqi Sun and
                  Xin Zhou},
  title        = {Better Zero-Shot Reasoning with Role-Play Prompting},
  journal      = {CoRR},
  volume       = {abs/2308.07702},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.07702},
  doi          = {10.48550/ARXIV.2308.07702},
  eprinttype    = {arXiv},
  eprint       = {2308.07702},
  timestamp    = {Wed, 23 Aug 2023 14:43:32 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-07702.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2412-05579,
  author       = {Haitao Li and
                  Qian Dong and
                  Junjie Chen and
                  Huixue Su and
                  Yujia Zhou and
                  Qingyao Ai and
                  Ziyi Ye and
                  Yiqun Liu},
  title        = {LLMs-as-Judges: {A} Comprehensive Survey on LLM-based Evaluation Methods},
  journal      = {CoRR},
  volume       = {abs/2412.05579},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2412.05579},
  doi          = {10.48550/ARXIV.2412.05579},
  eprinttype    = {arXiv},
  eprint       = {2412.05579},
  timestamp    = {Wed, 15 Jan 2025 21:22:49 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2412-05579.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{meneguzzi2008dalt,
  author       = {Felipe Rech Meneguzzi and
                  Michael Luck},
  editor       = {Matteo Baldoni and
                  Tran Cao Son and
                  M. Birna van Riemsdijk and
                  Michael Winikoff},
  title        = {Leveraging New Plans in AgentSpeak(PL)},
  booktitle    = {Declarative Agent Languages and Technologies VI, 6th International
                  Workshop, {DALT} 2008, Estoril, Portugal, May 12, 2008, Revised Selected
                  and Invited Papers},
  series       = {Lecture Notes in Computer Science},
  volume       = {5397},
  pages        = {111--127},
  publisher    = {Springer},
  year         = {2008},
  url          = {https://doi.org/10.1007/978-3-540-93920-7\_8},
  doi          = {10.1007/978-3-540-93920-7\_8},
  timestamp    = {Sun, 02 Jun 2019 21:16:10 +0200},
  biburl       = {https://dblp.org/rec/conf/dalt/MeneguzziL08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{silva2009atal,
  author       = {Lavindra de Silva and
                  Sebastian Sardi{\~{n}}a and
                  Lin Padgham},
  editor       = {Carles Sierra and
                  Cristiano Castelfranchi and
                  Keith S. Decker and
                  Jaime Sim{\~{a}}o Sichman},
  title        = {First principles planning in {BDI} systems},
  booktitle    = {8th International Conference on Autonomous Agents and Multiagent Systems
                  {(AAMAS} 2009), Budapest, Hungary, May 10-15, 2009, Volume 2},
  pages        = {1105--1112},
  publisher    = {{IFAAMAS}},
  year         = {2009},
  url          = {https://dl.acm.org/citation.cfm?id=1558167},
  timestamp    = {Tue, 05 Nov 2024 14:15:47 +0100},
  biburl       = {https://dblp.org/rec/conf/atal/SilvaSP09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cardoso2019emas,
  author       = {Rafael C. Cardoso and
                  Louise A. Dennis and
                  Michael Fisher},
  editor       = {Louise A. Dennis and
                  Rafael H. Bordini and
                  Yves Lesp{\'{e}}rance},
  title        = {Plan Library Reconfigurability in {BDI} Agents},
  booktitle    = {Engineering Multi-Agent Systems - 7th International Workshop, {EMAS}
                  2019, Montreal, QC, Canada, May 13-14, 2019, Revised Selected Papers},
  series       = {Lecture Notes in Computer Science},
  volume       = {12058},
  pages        = {195--212},
  publisher    = {Springer},
  year         = {2019},
  url          = {https://doi.org/10.1007/978-3-030-51417-4\_10},
  doi          = {10.1007/978-3-030-51417-4\_10},
  timestamp    = {Wed, 28 Sep 2022 08:59:53 +0200},
  biburl       = {https://dblp.org/rec/conf/emas/CardosoD019.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{schmid2024kgswc,
  author       = {Sebastian Schmid and
                  Michael Freund and
                  Andreas Harth},
  editor       = {Sanju Tiwari and
                  Boris Villaz{\'{o}}n{-}Terrazas and
                  Fernando Ortiz{-}Rodr{\'{\i}}guez and
                  Soror Sahri},
  title        = {Adaptive Planning on the Web: Using LLMs and Affordances for Web Agents},
  booktitle    = {Knowledge Graphs and Semantic Web - 6th International Conference,
                  {KGSWC} 2024, Paris, France, December 11-13, 2024, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {15459},
  pages        = {93--108},
  publisher    = {Springer},
  year         = {2024},
  url          = {https://doi.org/10.1007/978-3-031-81221-7\_7},
  doi          = {10.1007/978-3-031-81221-7\_7},
  timestamp    = {Tue, 25 Feb 2025 16:59:34 +0100},
  biburl       = {https://dblp.org/rec/conf/kgswc/SchmidFH24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ricci2024atal,
  author       = {Alessandro Ricci and
                  Stefano Mariani and
                  Franco Zambonelli and
                  Samuele Burattini and
                  Cristiano Castelfranchi},
  editor       = {Mehdi Dastani and
                  Jaime Sim{\~{a}}o Sichman and
                  Natasha Alechina and
                  Virginia Dignum},
  title        = {The Cognitive Hourglass: Agent Abstractions in the Large Models Era},
  booktitle    = {Proceedings of the 23rd International Conference on Autonomous Agents
                  and Multiagent Systems, {AAMAS} 2024, Auckland, New Zealand, May 6-10,
                  2024},
  pages        = {2706--2711},
  publisher    = {International Foundation for Autonomous Agents and Multiagent Systems
                  / {ACM}},
  year         = {2024},
  url          = {https://dl.acm.org/doi/10.5555/3635637.3663262},
  doi          = {10.5555/3635637.3663262},
  timestamp    = {Wed, 26 Jun 2024 14:06:50 +0200},
  biburl       = {https://dblp.org/rec/conf/atal/Ricci0ZBC24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{silvaBDIAgentArchitectures2020,
  title = {{{BDI Agent Architectures}}: {{A Survey}}},
  shorttitle = {{{BDI Agent Architectures}}},
  booktitle = {Proceedings of the {{Twenty-Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Silva, Lavindra De and Meneguzzi, Felipe and Logan, Brian},
  year = {2020},
  month = jul,
  pages = {4914--4921},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Yokohama, Japan},
  doi = {10.24963/ijcai.2020/684},
  urldate = {2025-06-10},
  abstract = {The BDI model forms the basis of much of the research on symbolic models of agency and agentoriented software engineering. While many variants of the basic BDI model have been proposed in the literature, there has been no systematic review of research on BDI agent architectures in over 10 years. In this paper, we survey the main approaches to each component of the BDI architecture, how these have been realised in agent programming languages, and discuss the trade-offs inherent in each approach.},
  isbn = {978-0-9992411-6-5},
  langid = {english},
}

@article{bestaGraphThoughtsSolving2024,
  title = {Graph of {{Thoughts}}: {{Solving Elaborate Problems}} with {{Large Language Models}}},
  shorttitle = {Graph of {{Thoughts}}},
  author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {16},
  eprint = {2308.09687},
  primaryclass = {cs},
  pages = {17682--17690},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v38i16.29720},
  urldate = {2024-11-05},
  abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-ofThought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (``LLM thoughts'') are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by {$>$}31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
}


@misc{yaoTreeThoughtsDeliberate2023a,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  year = {2023},
  month = dec,
  number = {arXiv:2305.10601},
  eprint = {2305.10601},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.10601},
  urldate = {2025-06-10},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
}

@online{xuLargeReasoningModels2025,
  title = {Towards {{Large Reasoning Models}}: {{A Survey}} of {{Reinforced Reasoning}} with {{Large Language Models}}},
  shorttitle = {Towards {{Large Reasoning Models}}},
  author = {Xu, Fengli and Hao, Qianyue and Zong, Zefang and Wang, Jingwei and Zhang, Yunke and Wang, Jingyi and Lan, Xiaochong and Gong, Jiahui and Ouyang, Tianjian and Meng, Fanjin and Shao, Chenyang and Yan, Yuwei and Yang, Qinglong and Song, Yiwen and Ren, Sijian and Hu, Xinyuan and Li, Yu and Feng, Jie and Gao, Chen and Li, Yong},
  date = {2025-01-23},
  eprint = {2501.09686},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.09686},
  url = {http://arxiv.org/abs/2501.09686},
  urldate = {2025-06-10},
  abstract = {Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of "thought" -- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to "think" with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI's o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
}

@online{valmeekamPlanningAbilitiesLarge2023a,
  title = {On the {{Planning Abilities}} of {{Large Language Models}} : {{A Critical Investigation}}},
  shorttitle = {On the {{Planning Abilities}} of {{Large Language Models}}},
  author = {Valmeekam, Karthik and Marquez, Matthew and Sreedharan, Sarath and Kambhampati, Subbarao},
  date = {2023-11-06},
  eprint = {2305.15771},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.15771},
  url = {http://arxiv.org/abs/2305.15771},
  urldate = {2025-06-10},
  abstract = {Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs in LLM-Modulo settings where they act as a source of heuristic guidance for external planners and verifiers. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of \textasciitilde 12\% across the domains. However, the results in the LLM-Modulo setting show more promise. In the LLM-Modulo setting, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
}

@misc{willardEfficientGuidedGeneration2023,
  title = {Efficient {{Guided Generation}} for {{Large Language Models}}},
  author = {Willard, Brandon T. and Louf, R{\'e}mi},
  year = {2023},
  month = aug,
  number = {arXiv:2307.09702},
  eprint = {2307.09702},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09702},
  urldate = {2025-06-10},
  abstract = {In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
}

@inproceedings{huangReasoningLargeLanguage2023,
  title = {Towards {{Reasoning}} in {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Towards {{Reasoning}} in {{Large Language Models}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2023},
  author = {Huang, Jie and Chang, Kevin Chen-Chuan},
  editor = {Rogers, Anna and {Boyd-Graber}, Jordan and Okazaki, Naoaki},
  year = {2023},
  month = jul,
  pages = {1049--1065},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.findings-acl.67},
  urldate = {2025-06-10},
  abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
}

@misc{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2023},
  month = jan,
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-26},
  abstract = {We explore how generating a chain of thought---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
}

@software{BAML,
  title = {BAML},
  author = {BoundaryML},
  url = {https://www.boundaryml.com/},
  abstract = {BAML is a simple prompting language for building reliable AI workflows and agents. BAML makes prompt engineering easy by turning it into schema engineering -- where you mostly focus on the models of your prompt -- to get more reliable outputs. You don't need to write your whole app in BAML, only the prompts! You can wire-up your LLM Functions in any language of your choice! See our quickstarts for Python, TypeScript, Ruby and Go, and more.},
  license = {Apache-2.0},
  year = {2023},
  repository = {https://github.com/boundaryml/baml}
}

@misc{tamLetMeSpeak2024,
  title = {Let {{Me Speak Freely}}? {{A Study}} on the {{Impact}} of {{Format Restrictions}} on {{Performance}} of {{Large Language Models}}},
  shorttitle = {Let {{Me Speak Freely}}?},
  author = {Tam, Zhi Rui and Wu, Cheng-Kuang and Tsai, Yi-Lin and Lin, Chieh-Yen and Lee, Hung-yi and Chen, Yun-Nung},
  year = {2024},
  month = oct,
  number = {arXiv:2408.02442},
  eprint = {2408.02442},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.02442},
  urldate = {2025-06-10},
  abstract = {Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs). This study investigates whether such constraints on generation space impact LLMs' abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs' performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs' reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
}

@online{wangComprehensiveSurveyLLM2024,
  title = {A {{Comprehensive Survey}} of {{LLM Alignment Techniques}}: {{RLHF}}, {{RLAIF}}, {{PPO}}, {{DPO}} and {{More}}},
  shorttitle = {A {{Comprehensive Survey}} of {{LLM Alignment Techniques}}},
  author = {Wang, Zhichao and Bi, Bin and Pentyala, Shiva Kumar and Ramnath, Kiran and Chaudhuri, Sougata and Mehrotra, Shubham and Zixu and Zhu and Mao, Xiang-Bo and Asur, Sitaram and Na and Cheng},
  date = {2024-07-23},
  eprint = {2407.16216},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2407.16216},
  url = {http://arxiv.org/abs/2407.16216},
  urldate = {2024-12-16},
  abstract = {With advancements in self-supervised learning, the availability of trillions tokens in a pre-training corpus, instruction fine-tuning, and the development of large Transformers with billions of parameters, large language models (LLMs) are now capable of generating factual and coherent responses to human queries. However, the mixed quality of training data can lead to the generation of undesired responses, presenting a significant challenge. Over the past two years, various methods have been proposed from different perspectives to enhance LLMs, particularly in aligning them with human expectation. Despite these efforts, there has not been a comprehensive survey paper that categorizes and details these approaches. In this work, we aim to address this gap by categorizing these papers into distinct topics and providing detailed explanations of each alignment method, thereby helping readers gain a thorough understanding of the current state of the field.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
}

@online{kojimaLargeLanguageModels2023,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  date = {2023-01-29},
  eprint = {2205.11916},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.11916},
  url = {http://arxiv.org/abs/2205.11916},
  urldate = {2025-06-10},
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
}

@online{mccoyWhenLanguageModel2024,
  title = {When a Language Model Is Optimized for Reasoning, Does It Still Show Embers of Autoregression? {{An}} Analysis of {{OpenAI}} O1},
  shorttitle = {When a Language Model Is Optimized for Reasoning, Does It Still Show Embers of Autoregression?},
  author = {McCoy, R. Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Mathew D. and Griffiths, Thomas L.},
  date = {2024-10-04},
  eprint = {2410.01792},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.01792},
  url = {http://arxiv.org/abs/2410.01792},
  urldate = {2024-12-16},
  abstract = {In “Embers of Autoregression” (McCoy et al., 2023), we showed that several large language models (LLMs) have some important limitations that are attributable to their origins in nextword prediction. Here we investigate whether these issues persist with o1, a new system from OpenAI that differs from previous LLMs in that it is optimized for reasoning. We find that o1 substantially outperforms previous LLMs in many cases, with particularly large improvements on rare variants of common tasks (e.g., forming acronyms from the second letter of each word in a list, rather than the first letter). Despite these quantitative improvements, however, o1 still displays the same qualitative trends that we observed in previous systems. Specifically, o1—like previous LLMs—is sensitive to the probability of examples and tasks, performing better and requiring fewer “thinking tokens” in high-probability settings than in low-probability ones. These results show that optimizing a language model for reasoning can mitigate but might not fully overcome the language model’s probability sensitivity.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
}

@online{mccoyEmbersAutoregressionUnderstanding2023,
  title = {Embers of {{Autoregression}}: {{Understanding Large Language Models Through}} the {{Problem They}} Are {{Trained}} to {{Solve}}},
  shorttitle = {Embers of {{Autoregression}}},
  author = {McCoy, R. Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Matthew and Griffiths, Thomas L.},
  date = {2023-09-24},
  eprint = {2309.13638},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.13638},
  url = {http://arxiv.org/abs/2309.13638},
  urldate = {2024-12-16},
  abstract = {The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations. We argue that in order to develop a holistic understanding of these systems we need to consider the problem that they were trained to solve: next-word prediction over Internet text. By recognizing the pressures that this task exerts we can make predictions about the strategies that LLMs will adopt, allowing us to reason about when they will succeed or fail. This approach—which we call the teleological approach—leads us to identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input. We predict that LLMs will achieve higher accuracy when these probabilities are high than when they are low—even in deterministic settings where probability should not matter. To test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven tasks, and we find robust evidence that LLMs are influenced by probability in the ways that we have hypothesized. In many cases, the experiments reveal surprising failure modes. For instance, GPT-4’s accuracy at decoding a simple cipher is 51\% when the output is a high-probability word sequence but only 13\% when it is low-probability. These results show that AI practitioners should be careful about using LLMs in low-probability situations. More broadly, we conclude that we should not evaluate LLMs as if they are humans but should instead treat them as a distinct type of system—one that has been shaped by its own particular set of pressures.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
}

@online{kambhampatiLLMsCantPlan2024,
  title = {{{LLMs Can}}'t {{Plan}}, {{But Can Help Planning}} in {{LLM-Modulo Frameworks}}},
  author = {Kambhampati, Subbarao and Valmeekam, Karthik and Guan, Lin and Verma, Mudit and Stechly, Kaya and Bhambri, Siddhant and Saldyt, Lucas and Murthy, Anil},
  date = {2024-06-12},
  eprint = {2402.01817},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2402.01817},
  urldate = {2024-11-05},
  abstract = {We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators. We present a vision of LLM-Modulo Frameworks that combines the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime. We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs. We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, extending the scope of modelbased planning/reasoning regimes towards more flexible knowledge, problem and preference specifications.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
}

@article{kambhampatiCanLargeLanguage2024,
  title = {Can {{Large Language Models Reason}} and {{Plan}}?},
  author = {Kambhampati, Subbarao},
  date = {2024-04},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Annals of the New York Academy of Sciences},
  volume = {1534},
  number = {1},
  eprint = {2403.04121},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {15--18},
  issn = {0077-8923, 1749-6632},
  doi = {10.1111/nyas.15125},
  url = {http://arxiv.org/abs/2403.04121},
  urldate = {2025-06-08},
  abstract = {While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
}

@online{wuReasoningRecitingExploring2024,
  title = {Reasoning or {{Reciting}}? {{Exploring}} the {{Capabilities}} and {{Limitations}} of {{Language Models Through Counterfactual Tasks}}},
  shorttitle = {Reasoning or {{Reciting}}?},
  author = {Wu, Zhaofeng and Qiu, Linlu and Ross, Alexis and Akyürek, Ekin and Chen, Boyuan and Wang, Bailin and Kim, Najoung and Andreas, Jacob and Kim, Yoon},
  date = {2024-03-28},
  eprint = {2307.02477},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.02477},
  url = {http://arxiv.org/abs/2307.02477},
  urldate = {2025-06-10},
  abstract = {The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
}

@misc{liuLostMiddleHow2023,
  title = {Lost in the {{Middle}}: {{How Language Models Use Long Contexts}}},
  shorttitle = {Lost in the {{Middle}}},
  author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  year = {2023},
  month = nov,
  number = {arXiv:2307.03172},
  eprint = {2307.03172},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.03172},
  urldate = {2024-12-16},
  abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
}

@online{zhangSurveyTestTimeScaling2025,
  title = {A {{Survey}} on {{Test-Time Scaling}} in {{Large Language Models}}: {{What}}, {{How}}, {{Where}}, and {{How Well}}?},
  shorttitle = {A {{Survey}} on {{Test-Time Scaling}} in {{Large Language Models}}},
  author = {Zhang, Qiyuan and Lyu, Fuyuan and Sun, Zexu and Wang, Lei and Zhang, Weixu and Hua, Wenyue and Wu, Haolun and Guo, Zhihan and Wang, Yufei and Muennighoff, Niklas and King, Irwin and Liu, Xue and Ma, Chen},
  date = {2025-05-04},
  eprint = {2503.24235},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.24235},
  url = {http://arxiv.org/abs/2503.24235},
  urldate = {2025-06-11},
  abstract = {As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q\&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions. Our repository is available on https://github.com/testtimescaling/testtimescaling.github.io/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
}

@article{shojaeeIllusionThinkingUnderstanding,
  title = {The {{Illusion}} of {{Thinking}}: {{Understanding}} the {{Strengths}} and {{Limitations}} of {{Reasoning Models}} via the {{Lens}} of {{Problem Complexity}}},
  author = {Shojaee, Parshin and Mirzadeh, Iman and Alizadeh, Keivan and Horton, Maxwell and Bengio, Samy and Farajtabar, Mehrdad},
  abstract = {Recent generations of frontier language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established mathematical and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from data contamination and does not provide insights into the reasoning traces’ structure and quality. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of compositional complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs “think”. Through extensive experimentation across diverse puzzles, we show that frontier LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having an adequate token budget. By comparing LRMs with their standard LLM counterparts under equivalent inference compute, we identify three performance regimes: (1) lowcomplexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity tasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks where both models experience complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across puzzles. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models’ computational behavior, shedding light on their strengths, limitations, and ultimately raising crucial questions about their true reasoning capabilities.},
  langid = {english},
}

@article{meijerVirtualMachinationsUsing2024,
  title = {Virtual {{Machinations}}: {{Using Large Language Models}} as {{Neural Computers}}: {{LLMs}} Can Function Not Only as Databases, but Also as Dynamic, End-User Programmable Neural Computers.},
  shorttitle = {Virtual {{Machinations}}},
  author = {Meijer, Erik},
  date = {2024-06-30},
  journaltitle = {Queue},
  shortjournal = {Queue},
  volume = {22},
  number = {3},
  pages = {25--52},
  issn = {1542-7730, 1542-7749},
  doi = {10.1145/3676287},
  url = {https://dl.acm.org/doi/10.1145/3676287},
  urldate = {2025-06-12},
  abstract = {We explore how Large Language Models (LLMs) can function not just as databases, but as dynamic, end-user programmable neural computers. The native programming language for this neural computer is a Logic Programming-inspired declarative language that formalizes and externalizes the chain-of-thought reasoning as it might happen inside a large language model.},
  langid = {english}
}

@article{DBLP:journals/ai/SlaneyT01,
  author       = {John K. Slaney and
                  Sylvie Thi{\'{e}}baux},
  title        = {Blocks World revisited},
  journal      = {Artif. Intell.},
  volume       = {125},
  number       = {1-2},
  pages        = {119--153},
  year         = {2001},
  url          = {https://doi.org/10.1016/S0004-3702(00)00079-5},
  doi          = {10.1016/S0004-3702(00)00079-5},
  timestamp    = {Tue, 24 Dec 2024 22:38:39 +0100},
  biburl       = {https://dblp.org/rec/journals/ai/SlaneyT01.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@online{holtzmanCuriousCaseNeural2020,
  title = {The {{Curious Case}} of {{Neural Text Degeneration}}},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  date = {2020-02-14},
  eprint = {1904.09751},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1904.09751},
  url = {http://arxiv.org/abs/1904.09751},
  urldate = {2025-06-24},
  abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language}
}

@article{omiciniArtifactsAAMetamodel2008,
  title = {Artifacts in the {{A}}\&{{A}} Meta-Model for Multi-Agent Systems},
  author = {Omicini, Andrea and Ricci, Alessandro and Viroli, Mirko},
  date = {2008-12-01},
  journaltitle = {Autonomous Agents and Multi-Agent Systems},
  shortjournal = {Auton Agent Multi-Agent Syst},
  volume = {17},
  number = {3},
  pages = {432--456},
  issn = {1573-7454},
  doi = {10/czjpkd},
  url = {https://doi.org/10.1007/s10458-008-9053-x},
  urldate = {2021-12-24},
  abstract = {In this article we focus on the notion of artifact for agents in multi-agent systems (MAS) as a basis for a new meta-model promoting the modelling and engineering of agent societies and MAS environment as first-class entities. Its conceptual foundations lay upon theories and results coming from computational sciences as well as from organisational and cognitive sciences, psychology, computer supported cooperative work (CSCW), anthropology and ethology. In the resulting agents \& artifacts (A\&A) meta-model, agents are the (pro-)active entities in charge of the goals/tasks that altogether build up the whole MAS behaviour, whereas artifacts are the reactive entities providing the services and functions that make individual agents work together in a MAS, and that shape agent environment according to the MAS needs. After presenting the scientific background, we define the notions of artifact in the A\&A meta-model, discuss how it affects the notion of intelligence in MAS, and show its application to a number of agent-related research fields.},
  langid = {english},
}

@software{Chase_LangChain_2022,
	author = {Chase, Harrison},
	month = oct,
	title = {{LangChain}},
	url = {https://github.com/langchain-ai/langchain},
	year = {2022}
}

@article{kuhnSurveyClassificationControlled2014,
  title = {A {{Survey}} and {{Classification}} of {{Controlled Natural Languages}}},
  author = {Kuhn, Tobias},
  date = {2014-03},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  volume = {40},
  number = {1},
  eprint = {1507.01701},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {121--170},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/COLI_a_00168},
  url = {http://arxiv.org/abs/1507.01701},
  urldate = {2025-06-25},
  abstract = {What is here called controlled natural language (CNL) has traditionally been given many different names. Especially during the last four decades, a wide variety of such languages have been designed. They are applied to improve communication among humans, to improve translation, or to provide natural and intuitive representations for formal notations. Despite the apparent differences, it seems sensible to put all these languages under the same umbrella. To bring order to the variety of languages, a general classification scheme is presented here. A comprehensive survey of existing English-based CNLs is given, listing and describing 100 languages from 1930 until today. Classification of these languages reveals that they form a single scattered cloud filling the conceptual space between natural languages such as English on the one end and formal languages such as propositional logic on the other. The goal of this article is to provide a common terminology and a common model for CNL, to contribute to the understanding of their general nature, to provide a starting point for researchers interested in the area, and to help developers to make design decisions.},
  keywords = {Computer Science - Computation and Language}
}

@software{ruslan_kuprieiev_2025_15646974,
  author       = {Ruslan Kuprieiev and
                  skshetry and
                  Peter Rowlands (변기호) and
                  Dmitry Petrov and
                  Paweł Redzyński and
                  Casper da Costa-Luis and
                  David de la Iglesia Castro and
                  Alexander Schepanovski and
                  Ivan Shcheklein and
                  Gao and
                  Batuhan Taskaya and
                  Jorge P and
                  Fábio Santos and
                  Daniele and
                  Ronan Lamy and
                  Aman Sharma and
                  Zhanibek Kaimuldenov and
                  Dani Hodovic and
                  Nikita Kodenko and
                  Andrew Grigorev and
                  Earl and
                  Nabanita Dash and
                  Dave Berenbaum and
                  George Vyshnya and
                  maykulkarni and
                  Max Hora and
                  Vera and
                  Sanidhya Mangal},
  title        = {DVC: Data Version Control - Git for Data \& Models},
  month        = jun,
  year         = 2025,
  publisher    = {Zenodo},
  version      = {3.60.1},
  doi          = {10.5281/zenodo.15646974},
  url          = {https://doi.org/10.5281/zenodo.15646974},
  swhid        = {swh:1:dir:a3548e0defd8ad12e7204cc48c1114915b9fa33c
                   ;origin=https://doi.org/10.5281/zenodo.3677553;vis
                   it=swh:1:snp:ba07a2bad72e043f2dabf1a35c5985faf32da
                   9a9;anchor=swh:1:rel:dd8a56dbd161890b973a1bac5c071
                   bfc524eb310;path=iterative-dvc-c7c7ba6
                  },
}

@article{muisePLANUTILSBringingPlanning,
  title = {{{PLANUTILS}}: {{Bringing Planning}} to the {{Masses}}},
  author = {Muise, Christian and Pommerening, Florian and Seipp, Jendrik and Katz, Michael},
  abstract = {PLANUTILS is a general library for setting up Linux-based environments for developing, running, and evaluating planners. Over the last decades, the planning community has produced countless solvers for various planning formalisms, as well as many other tools to help the planning practitioner. From state-of-the-art planners, over validators, to parsing libraries, the planning ecosystem has grown quite large. In the demo, we highlight an effort that aims to unify this ecosystem and make it seamless for users to get started with what the ICAPS community has to offer.},
  langid = {english}
}

@online{yangStructEvalBenchmarkingLLMs2025,
  title = {{{StructEval}}: {{Benchmarking LLMs}}' {{Capabilities}} to {{Generate Structural Outputs}}},
  shorttitle = {{{StructEval}}},
  author = {Yang, Jialin and Jiang, Dongfu and He, Lipeng and Siu, Sherman and Zhang, Yuxuan and Liao, Disen and Li, Zhuofeng and Zeng, Huaye and Jia, Yiming and Wang, Haozhe and Schneider, Benjamin and Ruan, Chi and Ma, Wentao and Lyu, Zhiheng and Wang, Yifei and Lu, Yi and Do, Quy Duc and Jiang, Ziyan and Nie, Ping and Chen, Wenhu},
  date = {2025-05-26},
  eprint = {2505.20139},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.20139},
  url = {http://arxiv.org/abs/2505.20139},
  urldate = {2025-06-28},
  abstract = {As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, a comprehensive benchmark for evaluating LLMs’ capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: (1) generation tasks, producing structured output from natural language prompts, and (2) conversion tasks, translating between structured formats. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gaps—even state-of-the-art models like o1-mini achieve only 75.58 average score, with open-source alternatives lagging approximately 10 points behind. We find generation tasks more challenging than conversion tasks, and producing correct visual content more difficult than generating text-only structures.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Software Engineering}
}

@online{longLLMsAreBiased2025,
  title = {{{LLMs Are Biased Towards Output Formats}}! {{Systematically Evaluating}} and {{Mitigating Output Format Bias}} of {{LLMs}}},
  author = {Long, Do Xuan and Ngoc, Hai Nguyen and Sim, Tiviatis and Dao, Hieu and Joty, Shafiq and Kawaguchi, Kenji and Chen, Nancy F. and Kan, Min-Yen},
  date = {2025-02-23},
  eprint = {2408.08656},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.08656},
  url = {http://arxiv.org/abs/2408.08656},
  urldate = {2025-06-28},
  abstract = {We present the first systematic evaluation examining format bias in performance of large language models (LLMs). 1 Our approach distinguishes between two categories of an evaluation metric under format constraints to reliably and accurately assess performance: one measures performance when format constraints are adhered to, while the other evaluates performance regardless of constraint adherence. We then define a metric for measuring the format bias of LLMs and establish effective strategies to reduce it. Subsequently, we present our empirical format bias evaluation spanning four commonly used categories—multiple-choice question-answer, wrapping, list, and mapping—covering 15 widely-used formats. Our evaluation on eight generation tasks uncovers significant format bias across state-of-the-art LLMs. We further discover that improving the formatinstruction following capabilities of LLMs across formats potentially reduces format bias. Based on our evaluation findings, we study prompting and fine-tuning with synthesized format data techniques to mitigate format bias. Our methods successfully reduce the variance in ChatGPT’s performance among wrapping formats from 235.33 to 0.71 (\%2).},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language}
}

@book{ProgrammingMultiAgentSystems2007,
  title = {Programming {{Multi-Agent Systems}} in {{AgentSpeak}} Using {{Jason}}},
  date = {2007},
  edition = {1},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9780470061848},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/9780470061848},
  urldate = {2025-07-02}
}

@software{Ip_deepeval_2025,
  author = {Ip, Jeffrey and Vongthongsri, Kritin},
  license = {Apache-2.0},
  month = jun,
  title = {{deepeval}},
  url = {https://github.com/confident-ai/deepeval},
  version = {3.2.3},
  year = {2025}
}

@online{liuGEvalNLGEvaluation2023,
  title = {G-{{Eval}}: {{NLG Evaluation}} Using {{GPT-4}} with {{Better Human Alignment}}},
  shorttitle = {G-{{Eval}}},
  author = {Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang},
  date = {2023-05-23},
  eprint = {2303.16634},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.16634},
  url = {http://arxiv.org/abs/2303.16634},
  urldate = {2025-07-04},
  abstract = {The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

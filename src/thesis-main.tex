\documentclass[12pt,a4paper,openright,twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{disi-thesis}
\usepackage{code-lstlistings}
\usepackage{notes}
\usepackage{shortcuts}
\usepackage{myacronyms}
\usepackage{epigraph} 
\usepackage{graphicx}
\usepackage[
    backref=true,
    backend=biber,
    style=numeric,
    sorting=none
  ]{biblatex}
\addbibresource{bibliography.bib}

\school{\unibo}
\programme{Laurea Magistrale in Ingegneria e Scienze Informatiche}
\title{Augmenting BDI agents with LLM-based plan generation: A Proof-of-Concept Study}
\author{Riccardo Battistini}
\date{\today}
\subject{Intelligent Systems Engineering}
\supervisor{Prof.\ Giovanni Ciatto}
\cosupervisor{Prof.\ Gianluca Aguzzi}
\session{I}
\academicyear{2024--2025}

\mainlinespacing{1.241}

\begin{document}

\frontmatter\frontispiece%

\begin{abstract}
Extending \acs{BDI} agents with the ability to autonomously generate plans has long been a goal in the field of cognitive agent engineering to enhance their adaptability.
%
Recent advances in \acs{GenAI} are now opening new possibilities for plan generation, by leveraging the natural-language understanding, means-end reasoning, and the abstraction capabilities of \acs{LLM}s.
%
This thesis investigates the integration of \acs{GenAI}-based plan generation into \agentspeak{} agents, and analyzes the implications of transferring knowledge between the \acs{LLM} and the \acs{BDI} agent for dynamic plan generation.
%
A framework is proposed that extends the reasoning cycle of \agentspeak{} agents with the capability to generate plans at runtime, and the design and implementation of the underlying generative process are discussed.
%
The framework is prototyped using the \jakta{} \acs{BDI} interpreter, and the quality of the plans generated by \acs{LLM}s of different sorts is methodologically assessed.
\end{abstract}

%----------------------------------------------------------------------------------------
\tableofcontents
\listoffigures     % (optional) comment if empty
\lstlistoflistings% (optional) comment if empty
%----------------------------------------------------------------------------------------

\mainmatter%

%----------------------------------------------------------------------------------------
\chapter{Introduction}\label{chap:introduction}
%----------------------------------------------------------------------------------------

Long before the current surge of interest in \ac{GenAI}, the \ac{BDI} model has worked as a paradigm for rational agents, leading to the development of several \ac{BDI} agent-oriented programming architectures~\cite{RaoG95}, as well as agent programming languages based on them---most notably, \agentspeak{}~\cite{rao-agentspeak96}.
%
\ac{BDI} languages promote the design of rational agents able in principle to reason about complex and dynamically evolving environments, and proactively act upon them.

Nevertheless, traditional \ac{BDI} programming frameworks typically lack mechanisms allowing agents to autonomously acquire or build new plans at runtime~\cite{SilvaSP09}.
%
The procedural behavior of \agentspeak{} agents is then limited to scenarios where the agent behavior can be suitably defined at design time (pre-programmed): an effective option in terms of computational efficiency, which however constrains agent flexibility, responsiveness, and overall \emph{autonomy} when facing unpredictable environments.
%
Extending \ac{BDI} agents with planning capabilities has thus been extensively explored~\cite{MeneguzziS15}.

Prior approaches typically attempt to bridge first-principles planning techniques with \ac{BDI} architectures~\cite{SilvaSP09}, requiring a detailed model of the action space.
%
However, those models, usually expressed in terms of preconditions and effects, are typically costly and often unavailable for highly-dynamic environments.

Recent advances in \ac{GenAI} methodologies promise to overcome such limitations by offering more dynamic and flexible solutions.
%
\ac{GenAI} technologies such as \acp{LLM} provide unprecedented opportunities for empowering cognitive agents with intelligent features involving the generation of cognitive abstractions (such as beliefs, goals, plans) as well as the ability to interact in natural language.

\paragraph{Implicit Hypothesis.} The exploration conducted in this thesis is grounded on the assumptions that:
%
\begin{inlinelist}
    \item \acp{LLM} exhibit planning capabilities~\cite{HuangLWCWL24};
    \item \acp{LLM} easily handle natural language, and have absorbed \emph{common sense} knowledge that can be leveraged in the planning process~\cite{llmoracles-kbs310};
    \item \acp{LLM} systems exhibit abstraction and \emph{imagination} capabilities---e.g.\,~\cite{AregbedeAPLL24}, which may lead to more flexible plans than those generated by classical planning techniques.
\end{inlinelist}
%
Furthermore, given that \ac{BDI} developers typically employ mnemonic and semantically-relevant names resembling natural language to represent cognitive abstractions, this work explores whether \acp{LLM} can generate \ac{BDI} plans by leveraging the natural-language semantics inherent in goals, actions, and plans, potentially reducing reliance on explicitly-defined preconditions and effects.

\paragraph{Goal of the thesis.} This work focuses on the problem of augmenting \ac{BDI} agents with autonomous plan generation capabilities by leveraging \ac{GenAI}. The investigation seeks to answer these four research questions:
%
\begin{enumerate*}[label=\textbf{(RQ\arabic*)}]
    \item\label{rq:required-info} \emph{what} information would \acp{LLM} require generating \ac{BDI} plans?
    \item\label{rq:knowledge-transfer} \emph{how} should knowledge be transferred between \acp{LLM} and \ac{BDI} agents?
    \item\label{rq:agent-spec} \emph{how} does automatic plan generation impact \ac{BDI} agents operation and specification?
    \item\label{rq:reusability} \emph{can} \acp{LLM} generate \emph{reusable} \ac{BDI} plans?
\end{enumerate*}
%
To address them, this thesis analyzes the integration requirements for incorporating generative AI into \ac{BDI} architectures, proposes a conceptual framework that extends the traditional \ac{BDI} model to support automated plan generation, and presents preliminary experimental results obtained through a prototype implementation in \jakta{}~\cite{JaktaSNCS2024}.

\paragraph{Structure of the thesis.} This work is organized as follows.~\Cref{sec:background} provides background on \ac{BDI} agents, reviews related work on planning in \ac{BDI} systems, and introduces \ac{GenAI} agents. 
%
\Cref{sec:design} presents the design of the generative process for runtime plan generation. 
%
\Cref{sec:implementation} offers a detailed examination of how \ac{GenAI}-based plan generation procedures are integrated into \agentspeak{} agents.
%
\Cref{sec:evaluation} discusses preliminary experimental results, while~\Cref{sec:conclusion} concludes the work and outlines directions for future research.

%----------------------------------------------------------------------------------------
\chapter{Background and Related Work}\label{sec:background}
%----------------------------------------------------------------------------------------

\section{Planning in \agentspeak{} agents}\label{sec:bdi-agents}

The \ac{BDI} model, inspired by human cognitive processes~\cite{BratmanEtAl1987}, and grounded upon a formal framework~\cite{bdilogic-jlc8} for modelling \emph{rational agents}' decision-making, has been adopted for agent programming languages---e.g.\,~\cite{BordiniHW2007}---as well as for simulating intelligent agents~\cite{HubnerB09} or human behavior~\cite{AdamGaudou2016}.
%
\ac{BDI} systems represent a wide yet specific class of \acp{MAS}, where rational agents feature \emph{cognitive} abstractions such as beliefs, goals, and intentions.

Specifically, a \ac{BDI} agent~\cite{RaoG95}:
%
\begin{inlinelist}
    \item maintains \emph{beliefs} about itself and its environment,
    updated by perception, reasoning, or communication with other agents;
    \item is guided by the \emph{desires} (or goals) it aims to achieve,
    which may evolve over time or give rise to new desires during their pursuit;
    \item commits to multiple concurrent \emph{intentions} as a means to fulfil its desires;
    \item executes \emph{plans}---as sequences of \emph{actions} it knows how to perform---in  order to achieve its goals.
\end{inlinelist}

Several architectures have been proposed to implement \ac{BDI} agents in software, including \agentspeak{}~\cite{RaoG95}, \ac{PRS}~\cite{IngrandGR1992}, and \ac{dMARS}~\cite{DInvernoLGKW04}.
%
The interested reader may refer to the comprehensive survey by Meneguzzi and de Silva~\cite{silvaBDIAgentArchitectures2020} for an account of the many architectures proposed in the literature.
%
In the following, the focus is on the widely-adopted \agentspeak{}---so that the terms \ac{BDI} agents and \agentspeak{} agents may be used interchangeably from now on.

\agentspeak{}~\cite{RaoG95} agents are animated by events, to which they respond by selecting \emph{plans} that guide their execution.
%
Events may correspond to the addition or removal of beliefs---which, in turn, may be the result of perception or communication with other agents---or to the occurrence of internal events, such as the commitment of the agent to a new goal to pursue.
%
Unlike purely \ac{ReAct}ive systems, where external stimuli are directly mapped to actions, \ac{BDI} agents reason about events in the context of their current state and choose \emph{plans} accordingly, via a continuous \emph{deliberation} activity.
%
The execution of plans can, in turn, generate new internal events, allowing the agent to progress through its reasoning cycle and operate autonomously.

\agentspeak{} agents perform so-called \emph{procedural reasoning}~\cite{IngrandGR1992}: they are equipped by human developers with procedural knowledge in terms of a \emph{plan library}, a collection of plans agents can select for execution at run time.
%
Agents' deliberation process is then aimed at selecting the most adequate plan for any event that may occur, so that plans are, ultimately, a way to encode specific actions to perform in response to specific events, failing if none has been provided.

More precisely, \agentspeak{} plans are referred to as \emph{plan rules}, denoted as:
%
\inlineAsl{
E\@: C ← A$_1$; \ldots; A$_n$.
}
%
The rule describes how the agent should \ac{ReAct} to some \emph{triggering event} \inlineAsl{E}, stating for instance what to do when a new percept arises, or how to achieve a given goal.
%
\inlineAsl{C} is the set of conditions under which the rule is applicable (a.k.a.\ the rule's \emph{context}), and \inlineAsl{A$_1$; \ldots; A$_n$} is the \emph{body} of the rule, corresponding to the actual plan, i.e., the actions to perform to handle the event.

Plan rules hence differ significantly from first-principles planning, where the plan is a sequence of actions to be performed to reach a desired world state---i.e., \emph{declarative} goals~\cite{winikoff2002kr}.
%
In fact, although it is possible to write plans rules and goals in a declarative style to mimic classic planning~\cite{hubner2006dalt, HindriksBHM00}, it is often the case that a goal expressed as the triggering event of a \ac{BDI} plan rule is simply a \emph{mnemonic name} of an event the agent may be willing to \ac{ReAct} to---i.e.,\ \emph{procedural} goals~\cite{winikoff2002kr}.
%
Additionally, other than encoding the procedural knowledge to achieve goals (\inlineAsl{!Goal}),
\agentspeak{} plan rules can be written to \ac{ReAct} to the addition or removal of beliefs (\inlineAsl{+/-Bel}), as well as performing \emph{epistemic actions}---e.g.,\ a \emph{test} to retrieve information from the current belief base (\inlineAsl{?Test}).
%
\begin{inlinelist}
    \item The addition (resp.\ removal) of a belief \inlineAsl{+Bel} (resp.\ \inlineAsl{-Bel})
    representing some novel (resp.\ old) information which the agent memorized (resp.\ forgot)
    due to reasoning, perception, or communication; or
    \item the commitment to a new achievement (resp.\ test) \emph{goal} \inlineAsl{+!G} (resp.\ \inlineAsl{+?G}),
    representing something that the agent intends to do (resp.\ know).
\end{inlinelist}
%
Practically speaking, while beliefs are logic (most commonly, Prolog) facts,---i.e.\ the agents' belief base is considered as a logic theory---such as \inlineAsl{temperature(25, celsius)}, goals are logic terms named after the activities they represent, such as \inlineAsl{!go(home)} or \inlineAsl{?weather(raining)}.
%
Plans events may be non-ground, in which case the plan would \ac{ReAct} to any event matching its head via logic unification---e.g.,\ the plan \inlineAsl{+temperature(X, celsius): tolerance(T) \& (X > T) ← !go(away)} would match any update concerning the temperature.
%
As shown in the last example, plans' contexts are just logic expressions possibly involving
the variables in the plan's head, or any other belief from the agent's belief base.
%
The many activities in a plan's body can either be
%
\begin{inlinelist}
    \item subgoals, i.e., expressions such as \inlineAsl{+!G} or \inlineAsl{+?G},
    representing new goals to be achieved (or tested) by the agent;
    \item actions, i.e., logic terms denoting operations to be performed,
    such as \inlineAsl{move(up)}; or
    \item special actions for the update of believes (e.g.\ \inlineAsl{+Bel} or \inlineAsl{-Bel}). \end{inlinelist}
%
Actions can, and most commonly will, provoke side effects when executed, i.e.\ change either the agent's internal status or affect the environment.

Similarly to goals, actions are basically \emph{mnemonic names} of functionalities causing effects.
%
Unlike other \ac{AI} subfields, though, pre-/post-conditions for an action are not explicitly represented in \agentspeak{} as the actions are usually invoking low-level software/hardware facilities that bring side effects about.
%
Typically, in \ac{BDI} agents, actions may even fail due to unpredictable environment dynamics making pre-/post-conditions hard to define.

Lastly, a notable feature of \agentspeak{} plan rules is that the \emph{body} of a plan may contain subgoals---i.e., statements triggering the selection and execution of other plans.
%
Through this mechanism, \agentspeak{} agents can pursue complex goals by breaking them down into smaller subgoals for which the specific plans are selected at runtime, resulting in a non-linear execution.
%
Yet, the underlying assumption is that all plans are known in advance, and the agent can select them based on the current context.
%
Then, it may happen that a plan is not available for a given event and context, in which case the agent may fail to achieve the goal.
%
To make that possible, \agentspeak{} assumes that low-level software/hardware facilities are attached to the agent, and that executing the action will invoke them accordingly.
%
So, in a sense, actions are invocations to \ac{FLI}.

When encountering a sub-goal during plan execution, a new internal event is generated inside the agent, which may trigger the execution of another plan, and so on recursively.
%
Stacks of goals, subgoals, and partially executed plans are called \emph{intentions}.
%
In a given moment, each agent may have multiple intention, each one keeping track of some course of action the agent is currently carrying on.

In this framework, goals are just \emph{mnemonic names} of events the agent may be willing to \ac{ReAct} to, and, similarly, actions are just \emph{mnemonic names} for functionalities which provoke side effects.
%
However, differently from other subfields of \ac{AI}, the actual semantics of goals and actions
(namely, what properties of the world should be reached for a goal to hold, or what pre-/post-conditions is an action subject to) is not explicitly represented in \agentspeak{}.
%
So, it is up to human developers to write \emph{meaningful} plans, goals, and actions, similarly to what programmers do with ordinary programming languages.

By supporting multiple concurrent intentions, \ac{BDI} agents can manage complex scenarios where multiple goals must be pursued in parallel, and where multiple courses of actions must be interleaved to pursue them all.
%
Yet, the underlying assumption is that all plans are known in advance, and the agent can select them based on the current context.
%
It may happen that a plan is not available for a given event and context, in which case the agent may fail to act, and the whole intention may remain unfulfilled.

\section{Plan Generation in BDI Agents}

This section highlights the main implications of integrating plan generation techniques into \ac{BDI} agents, which serves as a foundation for this work as well.

The main limitation of procedural reasoning in \ac{BDI} agents is that the plan library is statically defined by the agent programmer which has to anticipate all possible situations the agent may encounter.
%
Although this is beneficial for the agent's predictability and controllability, it limits the agent's flexibility and adaptability to unforeseen situations.
%
For this reason, the \ac{BDI} community has long been interested in the problem of \emph{automatic plan generation}, incorporating techniques from \ac{AI} planning into the \ac{BDI} architecture.
%
For an in-depth overview of the many approaches proposed in the literature so far, refer to the survey by Meneguzzi and de Silva~\cite{MeneguzziS15}.

The previous section surfaced the main differences between \ac{BDI} plans and classical (a.k.a.\ first-principles) planning (e.g.\, STRIPS~\cite{fikes1971ai}) which can be essentially be summarized as:
%
\begin{inlinelist}
    \item \ac{BDI} plans and goals are procedural, while in classical planning they are declarative~\cite{winikoff2002kr};
    \item \ac{BDI} actions have no clearly stated pre- and post-conditions, while classical planning actions are defined by them
    \item \label{item:diff-subgoals} \ac{BDI} plans can include subgoals for a more flexible execution flow, while classical planning is usually assembling a linear sequence of actions to move from one world state to another.
\end{inlinelist}
%
\ac{HTN} planning~\cite{georgievski2015ai}, based on the idea of task decomposition, allows generating more abstract plans that can mimic the invocation of subgoals addressing~\ref{item:diff-subgoals}.
%
Nevertheless, tasks must be defined in advance, have pre- and post-conditions and potentially ordering constraints.
%
Interestingly, as emerging from~\cite{MeneguzziS15}, classic planning techniques have mostly been integrated to generate new plans, while \ac{HTN} have been exploited to perform look-ahead in the plan library and understand if a suitable chain of plans is available.

Regardless of the specific planning technique, integrating plan generation in \ac{BDI} agents requires deciding \emph{when} to trigger the generation process.
%
The most straightforward approach is to trigger the generation process when the agent is unable to find a suitable plan in its library.
%
Another common approach is to trigger the generation through an agent action that can be invoked intentionally by the agent programmer as part of a plan as introduced in~\cite{meneguzzi2008dalt}.
%
The plans generated can then either be used to extend the original plan library, or be executed immediately (or both).
%
In the first case, plans should ideally be reused in the future~\cite{silva2009atal}, although this may lead to a bloated plan library and conflicts with the original plans that need to be handled~\cite{cardoso2019emas}.

\section{Reasoning and Planning in LLMs}

LLMs demonstrate remarkable proficiency in performing complex mathematical and logical operations, suggesting sophisticated reasoning abilities~\cite{huangReasoningLargeLanguage2023}.
%
The introduction of \ac{CoT} prompting has further amplified these capabilities by encouraging models to break down complex problems into intermediate reasoning steps, leading to substantial improvements in performance on challenging tasks~\cite{weiChainofThoughtPromptingElicits2023, kojimaLargeLanguageModels2023}.
%
However, while LLMs exhibit impressive problem-solving behavior, questions persist on the degree of robustness and generality of such abilities, both for what concerns \acp{LLM} and \acp{LRM}~\cite{mccoyEmbersAutoregressionUnderstanding2023, mccoyWhenLanguageModel2024, wuReasoningRecitingExploring2024}. 

Recent work by Shojaee et al.\ highlights a critical limitation of \ac{LLM} reasoning by systematically analyzing performance under controlled complexity in symbolic problem domains such as Tower of Hanoi and Blocks World~\cite{shojaeeIllusionThinkingUnderstanding}.
%
They identify a three-phase behavior in \acp{LRM}: dominance over non-thinking models on moderately complex tasks, regression on simpler tasks due to ``overthinking'', and total failure as problem complexity escalates. Despite models having computational budget, they reduce reasoning effort under higher complexity, revealing a scalability ceiling in inference-time reasoning.

Complementing these findings, Kambhampati et al.\ argue that LLMs fundamentally lack planning and self-verification capacities due to their architectural limitations and reliance on ``approximate retrieval'' of similar reasoning patterns in their training data~\cite{kambhampatiCanLargeLanguage2024, kambhampatiLLMsCantPlan2024}.
%
Through empirical evidence from PlanBench and planning domain benchmarks, they show that even the most advanced models fail to generate executable plans or reliably critique their outputs. 
%
In experiments with the Blocks World domain, \acp{LLM} were tasked with generating valid action sequences to reach specific goal configurations.
%
Despite using various prompting techniques (zero-shot, one-shot, Chain-of-Thought), success rates remained modest; for instance, GPT-4 achieved only 34.3\% correctness in one-shot mode.
%
The performance degraded significantly when the task was presented in a more syntactically obfuscated variant called Mystery Blocks World, where action and object names were replaced with arbitrary tokens.
%
In this case, even top-performing models dropped to near-zero accuracy (e.g.\, GPT-4 scored 4.3\% in one-shot and 0.16\% in zero-shot).
%
These results support the hypothesis that LLMs often rely on surface-level pattern matching and approximate retrieval rather than performing true reasoning or plan synthesis. 
%
The drastic performance collapse in the obfuscated version indicates a brittle dependence on lexical familiarity and highlights the lack of robust abstraction capabilities essential for planning.
%
As an alternative approach, they propose the \textit{LLM-Modulo} framework, which treats LLMs as approximate knowledge sources embedded in a generate-test-critique loop with external verifiers. This hybrid neuro-symbolic approach allows LLMs to play constructive roles in reasoning pipelines.

These works suggest that while LLMs can mimic certain aspects of reasoning behavior---particularly when supported by carefully designed prompts---they do not possess robust, generalizable reasoning or planning abilities.
%
Their effectiveness appears to depend significantly on context, complexity, and integration with structured external systems capable of ensuring correctness and coherence.

Given the ongoing debate surrounding the nature and reliability of \ac{LLM} reasoning and planning capabilities, this thesis adopts a pragmatic stance: it builds on established best practices in prompt engineering and assumes that the current planning and reasoning capabilities of \acp{LLM} are adequate for generating plans within the context of BDI agent architectures.
%
Nonetheless,~\Cref{sec:conclusion} outlines potential avenues for extending this framework in future work, particularly through the introduction of feedback loops---drawing inspiration from the LLM-Modulo paradigm---to enhance the robustness and reliability of plan generation.

\section{GenAI Agents}

Recent advances in \ac{GenAI} have sparked significant interest in developing agents that incorporate \acp{LLM}.
%
These systems leverage \acp{LLM} as cognitive engines, utilizing their natural language understanding, reasoning, planning, and knowledge recall abilities to create agents capable of complex, autonomous behavior~\cite{ParkOCMLB23, SumersYN024, WangMFZYZCTCLZWW24}. 
%
The remarkable ability of \acp{LLM} to perform effectively across diverse domains without task-specific training has made them ideal foundations for generative agents and autonomous systems~\cite{BubeckCEHLKLPZ23}.
%
In these agentic architectures, \acp{LLM} serve as the central ``brain'' that processes information, formulates plans, and coordinates actions. 
%
This integration enables the emergence of sophisticated behaviors that extend far beyond simple text generation, encompassing goal-directed planning, multistep reasoning, and adaptive problem-solving.
%
The field has witnessed the development of various paradigms, including generative agents, agentic AI, and autonomous agents, that explore how \acp{LLM} reasoning can be harnessed for intelligent behavior~\cite{Murugesan25a}.
%
Several approaches have emerged within this broad category.

One common approach focuses on creating ``general-purpose agents''~\cite{Murugesan25a} capable of executing actions, managing a memory, and using software tools with minimal human intervention.
%
Works following this approach, e.g.\ Auto-GPT~\cite{YangYH23}, demonstrate that \acp{LLM} can autonomously decompose goals into actionable steps.

Another approach aims to situate \acp{LLM} in physical environments.
%
For instance, Voyager~\cite{WangX0MXZFA24} represents a significant step towards lifelong learning agents by autonomously exploring the complex environment of Minecraft, acquiring new skills through interaction and feedback, and storing them in a skill library for future use.
%
Similarly, SayCan~\cite{HazraMR24} and PaLM-E~\cite{DriessXSLCIWTVY23} integrate \acp{LLM} with robotic control, using them to interpret high-level natural language instructions and generate action plans for execution.

Rather than proposing entirely new agent architectures, some works focus on enhancing the reasoning capabilities of existing ones.
%
The \ac{ReAct} framework~\cite{YaoZYDSN023} proposes a method for \acp{LLM} to synergize reasoning (generating thought traces) and acting (performing actions, e.g.\, tool use) in an interleaved manner, enabling agents to dynamically plan, execute, and adjust based on observations.
%
Similarly, Reflexion~\cite{ShinnCGNY23} introduces agents that can reflect on past failures,
maintain dynamic memory, and use self-reflection to improve their decision-making capabilities over time.
%
While these approaches focus primarily on enhancing reasoning capabilities, the generative agent architecture underlying ``Generative Agents''~\cite{ParkOCMLB23} provides a blueprint for single agents exhibiting believable, human-like, means-end reasoning capabilities.
%
These agents leverage memory structures (capturing past observations and deliberations) and \acp{LLM}-driven planning to simulate daily routines, form relationships, and generate emergent social dynamics based on their individual experiences and environment.

\section{Integration of BDI agents with LLMs}

Several approaches explore combining \acp{BDI} principles with \acp{LLM}.
%
The work by~\cite{IchidaM024} proposes NatBDI, a \ac{BDI} agent architecture designed for natural language environments.
%
It leverages \acp{LLM}, specifically for \ac{NLI}, within the \ac{BDI} reasoning cycle to determine the applicability of plans based on natural language beliefs and context descriptions, rather than generating the plans themselves.
%
Another direction, presented by~\cite{JangYCO023}, uses the \ac{BDI} structure itself to create structured prompts (``BDIPrompting'') for \acp{LLM}.
%
This guides the \ac{LLM} to generate more proactive and explainable task plans by framing the planning problem in terms of beliefs, desires, and intentions directly within the prompt given to the \ac{LLM}.

The approach presented in this thesis diverges by focusing on integrating generative plan generation into the well-established \agentspeak{}-based architecture. 
%
While much contemporary research prioritizes creating cognitive agents that directly leverage \ac{GenAI}—where agents \emph{are} generative systems—this work adopts a complementary perspective: strategically embedding generative capabilities within the \ac{BDI} framework rather than replacing it. 
%
This design preserves the controllability, predictability, and explainability of agent behavior while leveraging generative capabilities~\cite{ricci2024atal}.

Recent work has proposed enabling traditional agents to interrogate \acp{LLM} on-demand for plan adaptation in Web environments~\cite{schmid2024kgswc}. 
%
However, this work focuses on generating reusable plans from existing agent knowledge rather than relying on it to choose which actions to perform among those that are dynamically discovered at runtime.

The primary goal is investigating methodologies that enable \ac{BDI} agents to autonomously acquire novel plans, enhancing their operational autonomy and adaptive behavior. 
%
This thesis provides a proof of concept for the proposed approach, establishing groundwork for comprehensive future developments.

%----------------------------------------------------------------------------------------
\chapter{Generative Process Design}\label{sec:design}
%----------------------------------------------------------------------------------------

\section{The structure of a PGP}\label{sec:pgp-structure}

This section explores how \ac{BDI} agents can leverage \acp{LLM} to \emph{dynamically} synthesize actionable plans tailored to their specific goals. 
%
This capability is enabled by endowing each agent with a \emph{\acf{PGP}}, which the agent can invoke to generate new plans and incorporate them into its plan library.

Conceptually speaking, such a \ac{PGP} involves
%
\begin{inlinelist}
    \item meticulously encoding the agent’s current cognitive state and operational context---specifically: available actions, existing sub-plans, comprehensive beliefs, and the active goals the agent seeks to accomplish --into a structured prompt comprehensible to the \ac{LLM}, and \item parsing the \ac{LLM} output to extract the generated plans.
\end{inlinelist}
%
As discussed later, the implementation of this \ac{PGP} is non-trivial, as it must ensure that the generated plans are not only syntactically valid, but also coherent and contextually relevant, and obviously effective and possibly efficient in achieving the goal(s) they have been generated for.
%
However, before delving into the details of this implementation, the question of \emph{how} such a \ac{PGP} should work must first be addressed, along with \emph{when} it should be triggered (cf.~\Cref{sec:pgp-when}), and how the overall \agentspeak{} architecture should be adapted to accommodate generative capabilities.

The \ac{PGP} functionality works as follows: it accepts a goal \inlineAsl{G} as input (be it an achievement \inlineAsl{!g} or a test \inlineAsl{?G} goal), and it returns a non-empty set of plans $\mathcal{P}$, $\equiv { p_1, \ldots, p_m }$, such that at least one plan $p \in \mathcal{P}$ handles the event \inlineAsl{+G}.
%
The plans in $\mathcal{P}$ are subject to the following constraints:
%
\begin{enumerate*}[label=\textbf{(C\arabic*)}]
    \item each generated plan $p \in \mathcal{P}$ must handle different contexts (i.e., there should never be two plans $p,p' \in \mathcal{P}$ such that $p\neq p'$ yet both have the triggering event \inlineAsl{E} and context \inlineAsl{C});
    \item\label{item:allowed-refs} the generated plans' contexts and bodies \emph{should} refer\footnotemark{} to other goals or beliefs that are already known to the agent, if these help pursuing the goal \inlineAsl{G};
    
    \footnotetext{
        In the context of \agentspeak{}, plan contexts may \emph{refer} to beliefs, for instance to check whether a certain condition holds.
        %
        Similarly, plan bodies may \emph{refer} to {(sub-)}goals, for instance, because the achievement of the {(sub-)}goal is a precondition for the accomplishment of the plan.
        %
        In this work a belief is considered as already known' to the agent, if it is present in the agent's belief base at run time. 
        %
        Analogously, a goal is considered as already known' to the agent, if the agent's plan library has a plan that handles the goal.
    }

    \item\label{item:novel-refs} yet they may also reference novel, unknown goals or beliefs;

    \item\label{item:forbidden-refs} the generated plans' bodies \emph{must not} refer to actions\footnotemark{} which are unknown to the agent.

    \footnotetext{
       Plans bodies may also refer to actions.
       %
       Similarly to subgoals, these are activities to be performed by the agent to accomplish the plan.
       %
       Yet, differently than subgoals, actions are ``executed'' and when that happens, they affect the environment via actuators.
       %
       Technically, they require some low-level implementation to be invoked by the agent,instructing the actuators accordingly.
    }
\end{enumerate*}

The rationale behind these constraints is straightforward: the \ac{PGP} should handle the goal it was triggered for, and it should reuse existing knowledge \emph{whenever possible}.
%
In particular, if the generated plans need to affect the environment, they should do so by leveraging the actions the agent is already aware of---which were supposedly provided by the agent programmer(s) in advance.
%
This implies that the \ac{PGP} should keep into account which and how much information is available to the agent, and use it in the generation process.
%
Yet, the \ac{PGP} should also be open to the generation of new knowledge in the form of beliefs, and goals.
%
This may open up to the possibility, for the agent, to reuse the \ac{PGP} multiple times,recursively generating hierarchies of interrelated plans.

\section{Triggering the generative process}\label{sec:pgp-when}

Taking inspiration from prior literature on plan generation in \ac{BDI} agents, two main approaches can be identified to trigger a \ac{PGP}, namely: the \ac{ReAct}ive and the proactive ones. 
%
Each approach comes with different assumptions and implications, concerning the role of the agents' programmer, the shape of the generated plans, and the autonomy of the agent.

\subsection{On-demand PGP}

The on-demand \ac{PGP} is modelled as an \emph{action} that the agent can invoke whenever it needs to generate a plan.
%
Technically speaking, the agent is assumed to have a built-in action \inlineAsl{generate\_plan(E)} in its plan library, which it can invoke whenever it needs to generate a plan for event \inlineAsl{E}.
%
As an ordinary \agentspeak{} action, \inlineAsl{generate\_plan} may appear in the body of any plan, and be exploited by programmers to build smarter agents.
%
The action would just require the agent to provide some actual event \inlineAsl{E} as input, and its effect would be the production of a plan of the form \inlineAsl{E ← C : \ldots}, where \inlineAsl{C} is the context of the plan, to be added to the invoking agent's plan library.
%
Any failure in the \ac{PGP} execution would be reported as a failure of the action, to be handled by the agent as it would do for any other action.
%
The underlying assumption here is that the agent's programmer is in charge of deciding when then agent should trigger the \ac{PGP}.
%
In fact, by writing ordinary \agentspeak{} plans, programmers may decide to employ the \inlineAsl{generate\_plan} action to govern what exact plan should be generated, and under which conditions.
%
Another consequence of this approach is that, by looking at an agent's plan library, one may easily identify which and how many goals are going to require plan generation.
%
This may be useful for debugging purposes, or to understand the agent's behavior in advance.

It is worth noticing that, as a consequence of constraint~\ref{item:allowed-refs}, the \ac{PGP} may generate plans that use the \inlineAsl{generate\_plan} action in their body.
%
So, generated plans may themselves trigger the \ac{PGP} to generate new plans, when executed.
%
This powerful feature allows the agent to hierarchically generate plans, possibly decomposing complex plans into simpler ones, and so on recursively---provided that the underlying \ac{LLM} is smart enough to understand when it is wise to postpone the generation of a (sub-)plan, and when it is better to generate it immediately.

\subsection{\ac{ReAct}ive PGP}

The \ac{ReAct}ive \ac{PGP} is implicitly triggered by the agent's control loop whenever an event \inlineAsl{E} occurs and the agent has no applicable plan for it.
%
So, an unknown event would trigger the \ac{PGP} instead of making the current intention fail.
%
If the \ac{PGP} fails, the current intention fails---in the same way as when the agent has no plan available; if the \ac{PGP} is succeeds, the agent would keep on executing its intention as if the plan was already available from start.

The \ac{ReAct}ive approach could be considered as a refinement of the on-demand one, where the \ac{PGP} is automatically triggered by the agent's control loop, rather than explicitly invoked by the agent's programmer.
%
Provided that agents are equipped with the aforementioned \inlineAsl{generate\_plan} action, one may consider implementing the \ac{ReAct}ive approach as shown in~\Cref{lst:recative-pgp}:
%
\begin{lstlisting}[label={lst:recative-pgp}, caption={One possible implementation of a \ac{ReAct}ive \ac{PGP}}, captionpos=b]
(*@$p_\mathit{genex}$ $\equiv$@*) -G : missing_plan_for(G) <- generate_plan(G); G.
\end{lstlisting}
%
This approach borrows \jason{}'s extended \agentspeak{} syntax (cf.~\cite{BordiniHW2007}) for failure handling plans\footnotemark{} to express a plan (namely, $p_{genex}$) \ac{ReAct}ing to the failure event provoked by the absence of a plan for goal \inlineAsl{G}, to which the agent should \ac{ReAct} by generating a plan for it, and finally pursue goal \inlineAsl{G} on more time.
%
The underlying assumption here is that agents are equipped by default with plan $p_\mathit{genex}$, as well as with a special belief \inlineAsl{missing\_plan\_for(G)}, which computes whether plans are missing in the agent's plan library for goal \inlineAsl{G}.
%
\footnotetext{
    \jason{} is an implementation of \agentspeak{} which extends the syntax in many ways, there including failure events of the form \inlineAsl{-G}, which occur when some plan for goal \inlineAsl{G} fails or none is available.
}

The \ac{ReAct}ive approach improves the on-demand approach by making the generation procedure automatic, and therefore implicit.
%
Neither the agent's programmer nor the \ac{LLM} writing the plans have to worry about explicitly triggering the \ac{PGP}: they just have to mention goals to purpose in the plans, and the agent will take care of triggering the \ac{PGP} whenever needed---and never more than that.
%
In fact, another benefit here is the minimality of \ac{PGP} executions: aside from explicit invocations, the \ac{PGP} is triggered only when strictly necessary.

Finally, the \ac{ReAct}ive approach as well supports the generation of hierarchies of plans, in a way which is similar to the example from~\ref{sec:pgp-example}.
%
In fact, as prescribed by constraint~\ref{item:novel-refs}, the \ac{PGP} may generate plans that refer to novel, unknown goals.
%
When met at run-time, the agent will trigger the \ac{PGP} again to generate a plan for the new goal, and so on recursively.
%
The main difference here is that the \ac{LLM} is not required to be aware of the recursive nature of the planning activity it is involved into.
%
So, it can focus on the problem suggesting the best subgoals and actions to be performed to pursue a given goal.

\subsection{Proactive \acs{PGP}}

In the proactive approach, the \ac{PGP} is triggered by an ad-hoc, background intention, which is assumed to be built-in in each agent.
%
The intention would be responsible for continuously monitoring the agent's internal state, looking for goals or events for which the agent has no plan, and triggering the \ac{PGP} for each of them.
%
In case that no plans are missing, the intention would simply suspend itself or do nothing, waiting for further plans or goals being added to the agent---as these may contain new goals or events which require plan generation.

Conceptually speaking, such an intention is never meant to terminate.
%
Rather, it should carry on the following operations in a loop:
%
\begin{inlinelist}
    \item select one event \inlineAsl{E} for which no plan is available in the agent's plan library;
    \item trigger the \ac{PGP} to generate a plan for \inlineAsl{E} on a new intention
   ---so to allow multiple, concurrent \acp{PGP} for different events---
    \item wait for the addition of plans or goals in the agent's internal state;
    \item repeat.
\end{inlinelist}
%
Borrowing again from \jason{} syntax, the proactive approach can be expressed as follows:
%
\begin{lstlisting}
(*@$g_\mathit{init\ }$ $\equiv$@*) !generate_plans.

(*@$p_\mathit{scan}$ $\equiv$@*) !generate_plans <-
        for (event(E) & missing_plan_for(E)) {
            !!pgp(E)
        }; !generate_plans.

(*@$p_\mathit{gen\ }$ $\equiv$@*) !pgp(E) <- generate_plan(E).
\end{lstlisting}
%
There, goal $g_\mathit{init}$ is one built-in initial goal of the agent, whose only purpose is to start the generation intention along with the agent.
%
Plan $p_\mathit{scan}$ is a scanning plan: it selects known events from the agent's internal state---via the special belief \inlineAsl{event(E)}, plus the aforementioned \inlineAsl{missing\_plan\_for} one---and triggers plan $p_\mathit{gen}$ on a new intention for each of them.
%
Plan $p_\mathit{gen}$, in turn, triggers the \ac{PGP} to generate a plan for the event \inlineAsl{E}, without executing it thereafter.
%
Any failure in plan $p_\mathit{gen}$ would not interrupt plan $p_\mathit{scan}$, as it is running on a different intention.

Similarly to the \ac{ReAct}ive approach, the proactive one allows for the generation of hierarchical plans, without requiring any explicit action by either the agent's programmer or the \ac{LLM}.
%
Yet, by anticipating the \ac{PGP} execution, it may also mitigate the risk of the agent being stuck waiting for the \ac{PGP} to terminate, while also being in \emph{urgent} need of a plan to know how to act.
%
The drawback however is that race conditions may occur, for instance between an intention needing some missing plan, and the intention in charge of generating it.
%
So, it may happen that one intention fails out of a missing plan, because the intention in charge of generating it has not been executed yet.
%
This is a problem that may be solved by the agent's programmer, at the expense of additional coding efforts.

\section{Concurrency and PGP}

\note{
    Existing literature on integrating classical planners with BDI agents also mentions other issues related to concurrency, like how to handle conflicts, etc.
    Might add at least a reference.
}

Regardless of how/when the \ac{PGP} is triggered, any implementation approach should take into account that the \ac{PGP} is inherently \emph{blocking} for the agent.
%
In fact, at the current state of technology, \ac{LLM} are commonly queried via Web services,---to be contacted over the Internet --, and they may take up to many seconds to complete their response (or even fail to respond at all, e.g.\ due to network issues).
%
Even by assuming that the \ac{BDI} agent and the \ac{LLM} are running on the same machine, the \ac{LLM} may require entire seconds to generate a plan.
%
Such large time scales may be unacceptable for \ac{BDI} agents, especially if waiting for the \ac{PGP} to terminate implies blocking the agent's control loop.
%
This would hinder the agents' ability to \ac{ReAct} to events in a timely manner, and therefore its autonomy.

To mitigate this problem, implementers should consider \emph{suspending} only the intention that triggered the \ac{PGP}, rather than the whole control loop of the agent.
%
As multiple intentions may be concurrently carried on by an \agentspeak{} agent, this would allow it to ``keep thinking about how to \ac{ReAct} to event \inlineAsl{E}, while doing something else''.

\section{Transferring Knowledge between a BDI agent and an LLM}\label{sec:remarks}

This section delves into the practical aspects of endowing \ac{BDI} agents with a \ac{PGP},---most notably: how to effectively engineer the prompts for \ac{LLM} to let it generate plans, and how to write agent specifications in \agentspeak{}, to enrich them with additional knowledge that can be exploited by the new generative capabilities.

\subsection{From \acs{BDI} Agents to \acp{LLM} and Back}

In practice, how should the \ac{PGP} operate to work as discussed so far?

Technically, when triggered for a goal \inlineAsl{G}, the \ac{PGP} should:
%
\begin{inlinelist}
    \item convert the agent's internal state, as well as \inlineAsl{G}, into a textual prompt
   ---which is then used to query some \ac{LLM} of choice --,
    then
    \item parse the \ac{LLM} response to extract the generated plans.
\end{inlinelist}
%
The specific \ac{LLM} chosen does not impact the design of the \ac{PGP} itself, yet it may affect the quality of the generated plans---this is further discussed in~\Cref{sec:experiments}.

So, \emph{prompt generation} (structured information into free text) and  \emph{parsing} (free text back into structured information) are the two main non-trivial, \emph{complementary} activities of \ac{PGP}.
%
They may be implemented in several ways, mostly differing in terms of \emph{what to encode}, and \emph{how to encode} it, in the \ac{LLM} prompt and in the response.

\subsection{What to Encode}\label{sec:pgp-encoding}

This section describes the \ac{PGP} backwards, from its end back to its start, to better understand the rationale behind the proposed encoding.

\subsubsection{The Response}
%
The \ac{PGP} should produce a set of \ac{BDI} plans by querying some \ac{LLM},
which in turn would produce a textual response.
%
The response should therefore represent a (possibly empty) collection of plans,
where each plan should involve:
%
\begin{inlinelist}
    \item a triggering event,
    \item a context---i.e.\ a (possibly empty) collection of conditions to test --, and
    \item a body---i.e.\ a (possibly empty) collection of subgoals and actions.
\end{inlinelist}

\subsubsection{The Prompt}

\note{
    Tried to keep context length short, since LLMs exhibit performance deterioration with longer contexts, and information positioning within the prompt becomes increasingly critical~\cite{liuLostMiddleHow2023}, with text at the beginning and at the end being more relevant.
    (might not apply for the thousand tokens used for the prompt, since the study references longer contexts)
    Might be cited as an advantage of this PGP approach since it tries to avoid unchecked growth of the context, contrary to a \ac{ReAct}-inspired approach for example.
}

To support the generation of the aforementioned plans, the \ac{PGP} should provide all---and possibly only---relevant information to the \ac{LLM} via some \emph{automatically}-generated prompt, following a typical zero-shot prompting approach~\cite{KojimaGRMI22}, therefore encoding some instruction and information about the current agent's state in natural language and in structural form. This directly addresses~\ref{rq:required-info} (what information LLMs require).

The minimal set of relevant information might consist of:
%
\begin{enumerate*}[label=\textbf{(I\arabic*)}]
    \item\label{item:goal} the intended meaning of the goal \inlineAsl{G} for which the plans are needed---plus the explicit request to generate plans for it---,
    \item\label{item:admissible} the sorts of goals, actions, and beliefs which are already known to the agent,
    \item\label{item:current} the current plans, goals, and beliefs of the agent;
    as well as instructions on
    \item\label{item:pgp} what is the intended outcome of the plan generation process,
    \item\label{item:lang} the \agentspeak{} syntax and its intended meaning,
    \item\label{item:task} how to impersonate a \ac{BDI} agent willing to generate a plan
    (i.e., role-playing prompting~\cite{KongZCLQSSZ23}), and
    \item\label{item:bdi} what a \ac{BDI} agent is in the first place.
\end{enumerate*}

Whereas the \ac{LLM} needs~\ref{item:goal} to know what goal the agent is willing to pursue---namely, the goal \inlineAsl{!reach(home)} of the running example --~\ref{item:admissible} is required to know which goals, actions, or beliefs (akin to tool specifications in \ac{ReAct}~\cite{YaoZYDSN023}) the \ac{LLM} could use to generate the plans: e.g.\, some generated plans may check for obstacles in the surroundings of the agent via belief \inlineAsl{obstacle(Dir)}, even if the agent is currently not perceiving any obstacle.

\note{
    Add explanation of action \inlineAsl{move(Dir)}, as well as a general description of goals of the form \inlineAsl{!reach(Obj)}.
}

For the \ac{LLM} to account for the context currently perceived by the agent when generating the plans,~\ref{item:current} is needed.
%
For instance, beliefs such as \inlineAsl{there\_is(box, north\_east)}, \inlineAsl{obstacle(north)} should be included here.
%
~\ref{item:pgp} informs the \ac{LLM} about the purpose and the constraints of the plan generation process, as defined in~\Cref{sec:pgp}.
%
This is where, for instance, the possibility of formulate new goals and beliefs should be mentioned---along with the impossibility of creating new actions.

~\ref{item:lang} lets the \ac{LLM} comply with the \agentspeak{} syntax, when both reading the prompt and producing its response, and makes it aware of how cognitive abstractions are modelled in \agentspeak{}.
%
This is where, for instance, the \ac{LLM} is informed about the three parts of a plan (event, context, and body), and the components of a plan body (subgoals and actions).
%
Along with~\ref{item:bdi},---where the general notions of beliefs, desires, and intentions are provided---the core idea here is to provide the \ac{LLM} with enough background knowledge to operate in the \ac{BDI} domain, regardless of whether \ac{BDI} literature has been included in the training set of the \ac{LLM} or not.

Finally,~\ref{item:task} provide the \ac{LLM} with meta-level suggestions about how to devise plans in general.
%
There, recommendations such as ``decompose the task hierarchically'' or ``re-use the known goals and actions whenever useful'' could be included.

It is worth noticing that, while~\ref{item:goal},~\ref{item:admissible}, and~\ref{item:current} are specific to the agent's current state,~\ref{item:pgp},~\ref{item:lang},~\ref{item:task}, and~\ref{item:bdi} are more general, and could be factorized across different runs of the \ac{PGP}.

\subsection{How to Encode}\label{sec:prompt-encoding}

In principle flexibility in syntax is allowed for both the prompt and the response,
provided that
%
\begin{inlinelist}
    \item\label{syntax:prompt} the syntax used for the prompt is easily manipulable by the \ac{LLM},
    \item\label{syntax:response} the syntax used for the response is easily parsable by some non-\ac{LLM}-based parser.
\end{inlinelist}

\note{
    Put here the listing with a complete example of a prompt.
}

To achieve~\ref{syntax:prompt}, it is recommended to maximize the exploitation of natural language (as done in related works in \ac{LLM} as a ``planner''~\cite{Murugesan25a})----which does not mean discarding \agentspeak{} syntax, but rather complementing it with natural language descriptions.
%
For instance, when describing the goal \inlineAsl{!reach(Obj)} in the prompt, a natural language description like ``the agent wants to reach a situation where \inlineAsl{Obj} is perceived in its surroundings''.
%
Similarly, the admissible belief \inlineAsl{there\_is(Obj, Dir)} could be described as ``the agent can perceive the presence of \inlineAsl{Obj} in direction \inlineAsl{Dir}''.
%
Likewise, this applies to every admissible/current goal, belief, or action.

\note{
    Taking as reference~\cite{tamLetMeSpeak2024}, three main approaches can be considered for getting structured outputs out of a \ac{LLM} 
    -constrained generation techniques, based on CF grammars (i.e.\ JSON mode) or regexes, as seen in~\cite{willardEfficientGuidedGeneration2023};
    - format restricting instructions, which nudges the \ac{LLM} to generate responses in standardized formats, such JSON, XML and YAML;
    - NL-to-format: in which the \ac{LLM} is first instructed to answer the question in natural language, and then to convert its response into the target format schema.
    %
    The choice took into consideration the inherent trade-off between output structure compliance and generation performance~\cite{tamLetMeSpeak2024}.
    %
    Format restricting instructions to generate structured output in YAML format were chosen.
}

To achieve feature~\ref{syntax:response}, a description of the expected response syntax should be included in the prompt to let the \ac{LLM} know what is expected from it.
%
This approach is commonly referred to as \emph{structured output}~\cite{TangZPZCG23} and represents an important technique for controlling the format of \ac{LLM} responses.
%
\emph{Format restricting instructions} are used to nudge the \ac{LLM} to generate structured output. The chosen syntax defines a clear boundary between one generated plan and the next one is easily identifiable, and each section of the plan (event, context, operations) is clearly delimited.
%
This could be achieved by using the \agentspeak{} syntax directly, or, if the \ac{LLM} is not good at it---which is likely to be the case for small \acp{LLM}---by using a more renowned syntax, such as Prolog or YAML.
%
So, for instance, one may ask the \ac{LLM} to generate plans matching the following template:
%
\begin{lstlisting}
EVENT:
  (*@$\mathit{verb}$@*): event
CONDITIONS:
  - condition
  - other_conditon
  - ...
OPERATIONS:
  - (*@$\mathit{verb}$@*): operation1
  - (*@$\mathit{verb}$@*): operation2
  - ...
\end{lstlisting}
%
where $\mathit{verb}$ is a keyword among \inlineAsl{execute}, \inlineAsl{achieve}, \inlineAsl{test}, \inlineAsl{add}, \inlineAsl{remove}, and \inlineAsl{update}.
%
This enables easy parsing by any programming language while staying human-readable and debuggable,and easily generated by the \ac{LLM}---as it corresponds to well-known syntaxes (e.g.\, YAML).

\section{Writing Generative Agent Specifications}\label{sec:writing}

When \ac{BDI} agents are generative and can generate their own plans, one may wonder how the role of the programmer changes, to account for such new capabilities.
%
In other words, do agents still need programmers to write their plans?
%
Given the current state of technology, programmers are still needed to write the initial plans of the agent, and they are also required to provide hints about the intended semantics of the goals, beliefs, and actions they write.
%
The latter point in particular (writing hints) is the main focus of this section.
%
The aim is to show how developers can provide additional domain knowledge to the \ac{LLM} in an easily maintainable way, alongside the agent's specification, addressing~\ref{rq:agent-spec}.
%
This focuses on transferring knowledge to the \ac{LLM} to improve plan generation.

In the general case, it is not guaranteed that \acp{LLM} will be able to understand the intended meaning of the goals, beliefs, and actions they are asked to generate plans for/with.
%
This is because there is no guarantee that \ac{BDI} programmers would use intuitive and self-explanatory names for them, or, that the \ac{LLM} is trained on the \ac{BDI} programming language of choice.
%
For this reason, the encoding procedure (cf.~\Cref{sec:pgp-encoding}) is designed to include some natural-language description of each aspect of the agent's internal state.

Yet, descriptions cannot be automatically generated---as any automatic procedure may risk being inaccurate or incomplete w.r.t.\ the programmer's intent---and programmers would be better writing them directly.
%
Descriptions should be written in natural language and should explain the meaning of both the \emph{admissible} and \emph{actual} goals, beliefs, and actions each agent may have.

Accordingly, a \ac{PGP}-equipped \ac{BDI} agent requires one final assumption: agent programs should be written in a way that
\begin{inlinelist}
    \item natural-language descriptions for admissible and actual mental abstractions are part of the agent's code,
    and
    \item those descriptions are used to generate the prompt for the \ac{PGP}.
\end{inlinelist}
%
To this end, the \agentspeak{} framework, and consequently \ac{BDI} programming languages, should be extended with ad-hoc syntactical constructs for tagging cognitive abstractions with their intended meaning.
%
Furthermore, to minimize the programmer's burden, these constructs should be optional, and allow for templating and reusing descriptions.

Consider for instance the \inlineAsl{obstacle(Dir)} beliefs in~\Cref{fig:perceptions}.
%
When generating a \ac{LLM} prompt for them, the \ac{PGP} should
\begin{inlinelist}
    \item describe the general meaning beliefs of the form \inlineAsl{obstacle(Dir)}
   ---namely: ``an obstacle is perceived in direction \inlineAsl{Dir}''---and
    \item describe the specific meaning of each belief of this sort that the agent currently has
   ---namely: ``there is an obstacle in direction north''.
\end{inlinelist}
%
Yet, there is no need for the programmer to write all these descriptions by hand, as they can be automatically generated out of templates.

A proof-of-concept has been created in \jakta{}~\cite{JaktaSNCS2024}, an \agentspeak{}-based programming language implemented as a Kotlin \ac{DSL}---therefore easily extensible with additional syntactical constructs.
%
In \jakta{}, the new entries are (cf.~\Cref{app:agent-spec}):
\begin{inlinelist}
    \item the \texttt{.meaning\{\ldots\}} blocks, which can be used to tag \emph{actual} goals, beliefs, or actions with their intended meaning, via a post-fix syntax; and
    \item the \texttt{admissible\{\ldots\}} blocks, which can be used to tag \emph{admissible} goals or beliefs with their intended meaning, via a pre-fix syntax.
\end{inlinelist}
%
In both cases, the meaning is expressed as a string, attained via string interpolation, which in turn may include the functor, the arguments, and the presence/absence of negation of the tagged goal, belief, or action.
%
The strings are then exploited by the \ac{PGP} to generate the prompt for the \ac{LLM}.

\section{Alternative Approaches Considered}

This section reports on design choices that created several lines of work, all of which were later pruned for reasons discussed below.

\subsection{Alternative PGP}

\note{
    Argument why stateless and recursive.
}

The current generative process is stateless and recursive. What about a stateful approach that uses execution feedback to drive the generation?
%
Instead of executing each \ac{PGP} in a single fixed step as described in~\Cref{sec:pgp-structure}, this line of work proposes a dynamic multistep approach inspired by \ac{ReAct}~\cite{YaoZYDSN023}.
%
\ac{ReAct} demonstrates that prompting \acp{LLM} to generate interleaved reasoning traces and actions enables systems to dynamically reason about task execution while incorporating feedback from external environments.

This approach is adapted by allowing \acp{PGP} to take a variable number of steps, where each step involves generating reasoning about the current state and deciding on the next action. 
%
The key insight is to inform this generation process with feedback from the \ac{BDI} engine regarding whether previously requested plans executed successfully or failed. 
%
This creates a closed-loop system where the \ac{LLM} can adjust its planning strategy based on execution outcomes, similar to how \ac{ReAct} agents adapt their reasoning based on environmental feedback.
%
Whether this iterative approach converges to a set of plans that execute a non-trivial task provided by a \ac{BDI} agent developer remains to be proven.

\note{
    \acp{LLM} might be employed for plan repair and adaptative refinement.

    Employing the feedback on an external entity is not a new approach, and has been leveraged successfully to back-prompt the \ac{LLM} for better plan generation in several ways.
    %
    As an example Voyager uses the feedback from Minecraft to provide the \ac{LLM} with up-to-date information on how the environment changed in response to its chosen actions and whether syntax errors led to failure in executing plans~\cite{WangX0MXZFA24}. 
    %
    Similarly, in~\cite{valmeekamPlanningAbilitiesLarge2023a, kambhampatiLLMsCantPlan2024} it is shown that, in the context of the LLM-Modulo framework, using external components that act as verifiers or critics can improve the performance of language models.
    %
    The stance of Kambhampati et al.\ is exemplified by this quote: ''\acp{LLM} cannot plan themselves but can play a variety of constructive roles in solving planning tasks–especially as approximate knowledge sources and candidate plan generators in so-called LLM-Modulo Frameworks, where they are used in conjunction with external sound model-based verifiers``~\cite{kambhampatiLLMsCantPlan2024}.
}

\note{
    Highlight main disadvantages of a \ac{ReAct}-based approach for plan repair and refinement, namely: error propagation, inefficiency (context growth, token consumption, latency) and lack of context prioritization (lacks distinction between relevant and irrelevant information)
}

This approach was dismissed due to its limitations and excessive complexity, which would have expanded the research beyond the scope of this work.

\subsection{A Mechanism for Evaluating and Revising Plans}

Devised in the context of the alternative PGP discussed in the previous section.

\note{
    How to automatically assess whether a generate plan is good or not?
    Unification with belief base to check if logically encoded goal is achieved.
    Otherwise, if goal is expressed in natural language, ask the \ac{LLM} if it is possible to achieve a goal requested by the BDI engine and whether it is possible, if it can be considered completed given a set of plans and a relevant excerpt of the execution trace of one or more agents and of the environment.
    How to select relevant excerpts?
}

Discarded since it was out of scope.

\subsection{Alternative encoding format}

Devised in the context of the alternative PGP discussed in the previous section.

\note{
    Describe encoding of the prompt based on tags and the DSL for building regexes for writing specifications.
    Also,~\cite{meijerVirtualMachinationsUsing2024} since its Prolog encoding and decoding format inspired this one as well as the idea of intercepting the output of the \ac{LLM} as it is streamed, and the way tool calling is embedded in reasoning traces.
}

This design approach was discarded due to three significant limitations. 
%
First, the implementation would have discarded most of the \agentspeak{} syntax and deviated too far from its formalism.
%
Second, the prompt structure demanded many sequential reasoning steps, potentially compromising the performance due to the excessive complexity and length of the prompt. 
%
Third, this approach would have required constrained generation techniques to ensure the extraction of valid BDI syntax out of instances of the regexes in the output, introducing additional latency to \ac{LLM} responses and significant implementation complexity, both for what concerns the interaction with inference providers and the decoding algorithms themselves. 
%
Moreover, these techniques can degrade the performance of the \ac{LLM}, which is an inherent trade-off between output structure compliance and generation performance, as discussed in~\cite{tamLetMeSpeak2024}.
%
These constraints led to the exploration of alternative approaches to better preserve the BDI model semantics, respect the language model limitations and lower the implementation complexity and resulted in the choice of the YAML-based format outlined in~\Cref{sec:prompt-encoding}.

%----------------------------------------------------------------------------------------
\chapter{Generative Process Implementation}\label{sec:implementation}
%----------------------------------------------------------------------------------------

\section{Integration with the Deliberation Cycle}

\subsection{Declarative Plans}

\subsection{PGP Trigger Strategy}

\subsection{Tracking the Generated Goals}

\subsection{Handling Plan Unavailability}

\subsection{The Plan Invalidation Mechanism}

\section{The Generative Process Pipeline}

\subsection{Filterers}

\subsection{Formatters}

\subsection{Generators}

\subsection{Parsers}

\subsection{Requestors}

\section{Anatomy of a Generative Agent Specification}\label{sec:writing}

\subsection{Implementing Custom filters}

\subsection{Defining Prompt Templates}

\subsection{Choosing the Generation Strategy}

\subsection{Configuring the Generative Process}

\subsection{Providing Documentation}

\subsection{On-demand Generation Goals}

\subsection{Defining Custom Log Events}

%----------------------------------------------------------------------------------------
\chapter{Proof-of-Concept Evaluation}\label{sec:evaluation}
%----------------------------------------------------------------------------------------

\section{Experimental Setup}

The \ac{ReAct}ive \ac{PGP} approach described in~\Cref{sec:approach} is implemented within a \jakta{} agent~\cite{JaktaSNCS2024}, with specifications enriched through natural-language descriptions as outlined in~\Cref{sec:writing}. 

\note{
    The experiments span multiple \ac{LLM}s (\texttt{Gpt-4.1}, \texttt{Gemini Pro 2.5} (gemini-2.5-pro-preview), \texttt{Deepseek V3} (deepseek-chat-v3-0324), and \texttt{Claude 4 Opus}), with each model tested across five iterations to account for the stochastic nature of \ac{LLM} responses, yielding twenty total experimental runs.

    Update this according to the new experiments.

    Also add max tokens and temperature used and add prompt with doc vs prompt without doc.
}

\section{Evaluation Metrics}

To assess the quality of generated plans, the evaluation combines automatic and manual procedures.
%
This dual approach addresses both hard properties—such as syntactical correctness and functionality, which permit automatic testing—and soft properties including relevance, usefulness, and readability, which are difficult to formalize and automatically infer.
%
To evaluate the quality of generated plans, the following metrics are employed:
\newcommand{\PC}{\textbf{PC}}
\newcommand{\CC}{\textbf{CC}}
\newcommand{\PBC}{\textbf{PBC}}
\newcommand{\GR}{\textbf{GC}}
\newcommand{\RR}{\textbf{RR}}
\newcommand{\NGC}{\textbf{NGC}}
\newcommand{\NBC}{\textbf{NBC}}
\newcommand{\GSA}{\textbf{GSA}}
\newcommand{\BSA}{\textbf{BSA}}
\newcommand{\TSR}{\textbf{TSR}}
\newcommand{\GAT}{\textbf{GAT}}
\newcommand{\PRAS}{\textbf{PRAS}}
\begin{itemize}
    \item \emph{Plan Count} (\PC): Total number of generated plans.
    \item \emph{Context Complexity} (\CC): Average number of beliefs per plan context.
    \item \emph{Plan Body Complexity} (\PBC): Average number of operations per plan body.
    \item \emph{Generalization Count} (\GR): Number of generated plans that use variables (general) rather than constants (specific).
     \item \emph{Redundancy Amount} (\RR): Amount of generated plans which are useless (e.g.\, not executable at runtime or subsumed by more general plans).
    \item \emph{Novel Goal Count} (\NGC): Number of newly-invented goals.
    \item \emph{Novel Belief Count} (\NBC): Number of newly-invented beliefs.
    \item \emph{Goal Semantic Alignment} (\GSA): Number of semantically-misaligned uses of admissible goals.
    \item \emph{Belief Semantic Alignment} (\BSA): Number of semantically-misaligned uses of admissible beliefs.
    \item \emph{Task Success Rate} (\TSR): Percentage of experiments where the agent successfully achieves the \inlineAsl{!reach(home)} goal.
    \item \emph{Goal Achievement Time} (\GAT): Average time steps required to achieve the goal in successful experiments.
\end{itemize}
Among these metrics, \GR{} and \TSR{} should be maximized (higher values are better), while \RR{}, \GSA{}, \BSA{}, \GAT{} should be minimized (lower values are better).
%
The remaining metrics (\PC{}, \CC{}, \PBC{}, \NGC{}, and \NBC{}) have no clear optimization direction, yet they provide valuable comparative insights, with both extremely low and high values potentially indicating issues.

Additionally, to establish a comprehensive evaluation metric for plan quality, the \textbf{Plan-Reference Alignment Score (\PRAS{})} metric is introduced, to compare generated plans against expert-designed reference plans.
%
For this evaluation, an \ac{LLM} is employed as an explicit judge~\cite{DBLP:journals/corr/abs-2412-05579} to assess the generated plans across three key dimensions: quality of abstraction, generalizability, and adherence to BDI principles. 
%
The \ac{LLM} judge compares these generated plans against reference implementations to provide systematic evaluation---see~\Cref{app:llm-judge}.
%
This \ac{LLM}-as-judge methodology allows  to systematically rate plan quality using criteria that would be difficult to formalize through traditional metrics alone.
%
\PRAS{} tends to one when the generated plans are similar to the reference ones, to zero when they are completely different.

\section{LLM as a judge prompt}\label{app:llm-judge}
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,breaklines=true]
"Extract invented goals, beliefs, and plans from the 'actual output'.",
"Compare extracted plans against 'expected output' plans for logical equivalence and coverage.",
"Assess if invented goals/beliefs are necessary or add needless complexity compared to 'expected output'.",
"Evaluate plan minimality; penalize unnecessary subgoals, conditions, or operations vs 'expected output'.",
"Verify that operations correctly use specified prefixes (execute, achieve, add, etc.) and admissible actions.",
"Check if conditions logically correspond to the intended plan activation scenario.",
"Score based on plan correctness, necessity of inventions, and adherence to minimality principle."
\end{lstlisting}

\section{The explorer robot application}

The agent has to reach home within a simulated 2D grid world environment where it can perceive obstacles, with no pre-programmed knowledge.

\subsection{Agent Specification for Plan Generation}

In \jakta{}, the code of the agent from~\Cref{fig:environment-model} would be look like the one in~\Cref{lst:agent-spec}.
%
\lstinputlisting[
    % float,
    label={lst:agent-spec},
    caption={Example of a \jakta{} agent extended with natural-language descriptions},
    language=Kotlin,
    basicstyle=\footnotesize\ttfamily,
    captionpos=b,
    mathescape=false
]{listings/agent-spec.kt}

The environment is shown in~\Cref{lst:env-spec}.

\lstinputlisting[
    % float,
    label={lst:env-spec},
    caption={Example of a \jakta{} agent extended with natural-language descriptions},
    language=Kotlin,
    basicstyle=\footnotesize\ttfamily,
    captionpos=b,
    mathescape=false
]{listings/env-spec.kt}

Where the \verb|GridWorld| class is expected to generate beliefs in the form shown in~\Cref{lst:gridworld-beliefs}.

\begin{lstlisting}[
    caption={The set of percepts generated by the GridWorld environment.}
]
direction(north): North is a direction.
...
direction(south): South is a direction.

direction(here): here denotes the null direction w.r.t. the agent's current location.

object(home): Home is an object.
object(rock): Rock is an object.

obstacle(north): there is an obstacle to the North.
free(south): there is not obstacle to the South.

there_is(home, east): there is a Home to the East.
there_is(rock, west): there is a Rome to the West.
\end{lstlisting}

\subsection{\acs{PGP} Prompt}\label{app:prompt}

This section presents a complete example of a \ac{PGP} prompt that follows the structure discussed in~\Cref{sec:pgp-encoding}. 
%
The prompt incorporates natural language descriptions of goals, beliefs, and actions, while providing clear instructions on the expected response format.

\lstinputlisting[
    % float,
    label={lst:jakta-code},
    caption={Example of a \jakta{} agent extended with natural-language descriptions},
    language=Kotlin,
    basicstyle=\tiny\ttfamily,
    captionpos=b,
    mathescape=false
]{listings/prompt.md}
\subsection{\acs{PGP} Responses}\label{app:response}

This section examines the structured responses generated by the prompt detailed in~\Cref{app:prompt}. 
%
The examples demonstrate how \acp{LLM} can produce diverse plans for achieving the goal of reaching home, each adapted to different contextual scenarios.
%
While some plans adopt direct approaches to accomplish the objective, others exhibit greater generalizability by incorporating variables such as the capitalized placeholder \texttt{Object}.

\subsection{Example evaluation of a PGP response}\label{app:response}

\subsection{Experimental Results}

%----------------------------------------------------------------------------------------
\chapter{Conclusion}\label{sec:conclusion}
%----------------------------------------------------------------------------------------

In this thesis, the exploration focuses on integrating \ac{GenAI} into the \agentspeak{} agent architecture to generate new plans at runtime, based on the agent's current knowledge.
%
The goal is to assess if and how \acp{LLM} can generate plans for \ac{BDI} agents, potentially reducing reliance on human programmers or first-principle planners.

Unlike approaches that treat agents as generative systems themselves, this work preserves the strengths of the \ac{BDI} model---its theoretical foundation, programming paradigms, and controllability---while enhancing it with generative and \ac{NLP} capabilities for plan generation.
%
The approach encapsulates the \ac{LLM} within a \ac{PGP} functionality, responsible for generating plans \ac{ReAct}ively or on-demand, ensuring compatibility with the \ac{BDI} architecture.
%
This thesis defines the \ac{PGP} interface, proposes implementation guidelines, and evaluates a prototype implementation to validate the approach.

Addressing~\ref{rq:required-info}, the \ac{PGP} is defined to use structured prompts detailing current and generally \emph{admissible} goals and beliefs, available actions and plans, and the operational semantics of the \ac{BDI} architecture.

\note{
    Add that clear natural language semantics significantly improves the performance of the PGP,leading to practical and relevant plans, if experiments confirm this.
}

For~\ref{rq:knowledge-transfer}, structured prompts paired with parsable outputs ensure reliable integration of generated plans into the agent's library, balancing expressiveness with formal rigor.
%
The work further speculates that knowledge transfer can be improved by providing additional semantics to the \ac{LLM} via natural language descriptions of the agent's goals, beliefs, and actions.

Consequently, answering~\ref{rq:agent-spec}, automatic plan generation shifts the agent specification from exhaustive plan encoding to providing the basic (procedural) knowledge necessary for the problem domain, potentially leaving to the \ac{LLM} the task of handling corner cases and unexpected situations.
%
The agent operation stays the same of a classic \ac{BDI} agent, with the possibility of the \ac{PGP} to be triggered explicitly (on-demand) or implicitly (\ac{ReAct}ively) to handle goals with no matching plans.

For~\ref{rq:reusability}, the methodology outlined in this work shows promise for the generation of reusable, general plans involving variables, despite variability in \ac{LLM} performance.

\section{Future Work}

\note{
    To consider:
    - Artifacts as Tools
        - Expressing plans in a controlled natural language
        - Leveraging ontologies to ease knowledge transfer
        - Modularization of the deliberation cycle
        - Integration with MCP and RAG
        - Runtime validation and verification mechanisms
        - Plan repair, failure learning and adaptive refinement
        - Plan selection
        - Plan and goal recognition
        - Fine-tuning and Alignment of LMs: Despite the performance of SLM seems very low from preliminary results, fine-tuning and alignment have generally proved itself as a very powerful approach to boost the performance of SLM (missing citation).
        %
        Also add survey of alignment techniques like RLFH, DPO, PPO and GRPO~\cite{wangComprehensiveSurveyLLM2024}. 
        %
        May also add SFT, PEFT, LoRA.\@
        - Test Time Scaling: search algorithms and verification~\cite{zhangSurveyTestTimeScaling2025}
        - More complex testing scenarios
        - User studies: how the BDI agent programmer might benefit from this?
}

\note{
    \acp{LLM} compared to classical planners as way less efficient and unreliable due to stocasticity, should be used for suggesting candidate plans to be verified and learn from mistakes, not only integrated in the reasoning cycle like classical planners have been in previous \ac{BDI} agent literature. The current framework lacks support for verification, both at the design and implementation level.
}

Future work includes exploring runtime validation and verification mechanisms
to address hallucinations or inaccuracies in \ac{LLM}-generated constructs.
%
Studying prompt robustness under varying conditions and developer inaccuracies
could enhance the methodology's practicality.
%
Ablation studies may clarify factors influencing prompt effectiveness~\ref{rq:required-info}
and knowledge transfer~\ref{rq:knowledge-transfer}.
%
Testing scenarios with concurrent goals, dynamic environments, and partial observability
will validate scalability and generalization~\ref{rq:reusability}.
%
Finally, incorporating plan repair, failure learning, and adaptive refinement further enhances agent autonomy and long-term performance.

This research bridges traditional \ac{BDI} architectures and emerging \ac{GenAI} technologies,
laying the groundwork for more autonomous, adaptable, and explainable cognitive agents.

%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\backmatter%
\addcontentsline{toc}{chapter}{Bibliografia}
\printbibliography%

\end{document}
